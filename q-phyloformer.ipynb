{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12750561,"sourceType":"datasetVersion","datasetId":8060187},{"sourceId":12809530,"sourceType":"datasetVersion","datasetId":8090481},{"sourceId":532631,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":415819,"modelId":433565}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install biopython\n!pip install ete3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:53:44.856167Z","iopub.execute_input":"2025-08-27T05:53:44.856920Z","iopub.status.idle":"2025-08-27T05:54:01.825675Z","shell.execute_reply.started":"2025-08-27T05:53:44.856855Z","shell.execute_reply":"2025-08-27T05:54:01.824162Z"}},"outputs":[{"name":"stdout","text":"Collecting biopython\n  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\nCollecting ete3\n  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: ete3\n  Building wheel for ete3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273786 sha256=3bb59d617958b54154a83aba91bf2b0f1e19b79159b2ab3fbda55158e4213097\n  Stored in directory: /root/.cache/pip/wheels/dd/a8/60/0a29caa9f8ceb7316704be63c1578ab13c36668abb646366ac\nSuccessfully built ete3\nInstalling collected packages: ete3\nSuccessfully installed ete3-3.1.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport numpy as np\nimport functools\nimport argparse\nfrom Bio import AlignIO\nfrom itertools import combinations\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" \nimport math\nimport random\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Sequential, regularizers\nimport torch\nfrom ete3 import Tree\nfrom pandas.core.frame import DataFrame\n\nglobal org_seq, comb_of_id, qp_model, qp_predict, len_of_msa, dic_for_leave_node_comb_name, start_end_list\nglobal taxa_num, leave_node_id, leave_node_name, leave_node_comb_id, leave_node_comb_name, internal_node_name_pool\n\nfrom multiprocessing import Process, Pool\nimport multiprocessing\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\nimport random\nimport os\nfrom Bio import SeqIO, Phylo\nimport csv\nfrom itertools import combinations\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom torch.utils.data import random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:01.828021Z","iopub.execute_input":"2025-08-27T05:54:01.828341Z","iopub.status.idle":"2025-08-27T05:54:29.599400Z","shell.execute_reply.started":"2025-08-27T05:54:01.828311Z","shell.execute_reply":"2025-08-27T05:54:29.598407Z"}},"outputs":[{"name":"stderr","text":"2025-08-27 05:54:04.922320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756274045.306517      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756274045.405188      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"DEFAULT_RANDOM_SEED = 2021      # Please change this seed\nseed = DEFAULT_RANDOM_SEED\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.600330Z","iopub.execute_input":"2025-08-27T05:54:29.600854Z","iopub.status.idle":"2025-08-27T05:54:29.610865Z","shell.execute_reply.started":"2025-08-27T05:54:29.600830Z","shell.execute_reply":"2025-08-27T05:54:29.609942Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### [Fusang] Util functions\n- comb_math\n- nlargest_indices\n- get_quartet_ID","metadata":{}},{"cell_type":"code","source":"def comb_math(n,m):\n    return math.factorial(n)//(math.factorial(n-m)*math.factorial(m))\n\n\ndef nlargest_indices(arr, n):\n    uniques = np.unique(arr)\n    threshold = uniques[-n]\n    return np.where(arr >= threshold)\n\n\ndef get_quartet_ID(quartet):\n    return \"\".join(sorted(quartet))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.613182Z","iopub.execute_input":"2025-08-27T05:54:29.613506Z","iopub.status.idle":"2025-08-27T05:54:29.629775Z","shell.execute_reply.started":"2025-08-27T05:54:29.613479Z","shell.execute_reply":"2025-08-27T05:54:29.628819Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### [Fusang] Tree from quartet","metadata":{}},{"cell_type":"code","source":"def tree_from_quartet(quartet):\n    root = Tree()\n    root.name = \"internal_node_0\"\n    left = root.add_child(name=\"internal_node_1\")\n    left.add_child(name=quartet[0])\n    left.add_child(name=quartet[1])\n    right = root.add_child(name=\"internal_node_2\")\n    right.add_child(name=quartet[2])\n    right.add_child(name=quartet[3])\n    for desc in root.iter_descendants():\n        desc.dist = 0\n    return root","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.630746Z","iopub.execute_input":"2025-08-27T05:54:29.631049Z","iopub.status.idle":"2025-08-27T05:54:29.650455Z","shell.execute_reply.started":"2025-08-27T05:54:29.631021Z","shell.execute_reply":"2025-08-27T05:54:29.649405Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### [Fusang] Getting topology ID\n- Given a quartet\n- Given a quartet key and 2 clusters","metadata":{}},{"cell_type":"code","source":"def get_topology_ID(quartet):\n    return get_quartet_ID(quartet[0:2]) + get_quartet_ID(quartet[2:4])\n\n\ndef get_current_topology_id(quart_key, cluster_1, cluster_2):\n    ans = []\n    a1 = quart_key.index(cluster_1)\n    a2 = quart_key.index(cluster_2)\n    ans.append(str(a1))\n    ans.append(str(a2))\n    ans = set(ans)\n    if ans == {'0', '1'} or ans == {'2', '3'}:\n        return 0\n    elif ans == {'0', '2'} or ans == {'1', '3'}:\n        return 1\n    elif ans == {'0', '3'} or ans == {'1', '2'}:\n        return 2\n    else:\n        print('Error of function get_current_topology_id, exit the program')\n        sys.exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.651626Z","iopub.execute_input":"2025-08-27T05:54:29.652012Z","iopub.status.idle":"2025-08-27T05:54:29.679871Z","shell.execute_reply.started":"2025-08-27T05:54:29.651980Z","shell.execute_reply":"2025-08-27T05:54:29.678762Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### [Fusang] Tree Functions\n- judge_tree_score\n- get_modify_tree","metadata":{}},{"cell_type":"code","source":"def judge_tree_score(tree, quart_distribution, new_addition_taxa, dic_for_leave_node_comb_name):\n    '''\n        parameter\n        tree: a candidate tree, can be any taxas\n        quart_distribution: the prob distribution of the topology of every 4-taxa\n    '''\n    crt_tree = tree.copy(\"newick\")\n    leaves = crt_tree.get_leaves()\n\n    leaves = [ele.name for ele in leaves]\n    total_quarts = list(combinations(leaves, 4))\n    quarts = []\n    for ele in total_quarts:\n        if new_addition_taxa in ele:\n            quarts.append(ele)\n\n    total_quart_score = 0\n\n    for quart in quarts:\n        crt_tree = tree.copy(\"newick\")\n        try:\n            crt_tree.prune(list(quart))\n        except:\n            print('Error of pruning 4 taxa from current tree, the current tree is:')\n            print(crt_tree)\n            sys.exit(0)\n\n        quart_key = \"\".join(sorted(list(quart)))\n        #quart_topo_id = leave_node_comb_name.index(quart_key)\n\n        quart_topo_id = dic_for_leave_node_comb_name[quart_key]\n\n        quart_topo_distribution = quart_distribution[quart_topo_id]\n\n        # judge current tree belongs to which topology\n        tmp = re.findall(\"\\([\\s\\S]\\,[\\s\\S]\\)\", crt_tree.write(format=9))[0]\n        topology_id = get_current_topology_id(quart_key, tmp[1], tmp[3])\n\n        total_quart_score += np.log(quart_topo_distribution[topology_id]+1e-200)\n\n    return total_quart_score\n\n\ndef get_modify_tree(tmp_tree, edge_0, edge_1, new_add_node_name):\n    '''\n        add a new leave node between edge_0 and edge_1\n        default: edge_0 is the parent node of edge_1\n    '''\n    modify_tree = tmp_tree.copy(\"newick\")\n    if edge_0 != edge_1:\n        new_node = Tree()\n        new_node.add_child(name=new_add_node_name)\n        detached_node = modify_tree&edge_1\n        detached = detached_node.detach()\n        inserted_node = modify_tree&edge_0\n        inserted_node.add_child(new_node)\n        new_node.add_child(detached_node)\n\n    else:\n        modify_tree.add_child(name=new_add_node_name)\n\n    return modify_tree\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.681151Z","iopub.execute_input":"2025-08-27T05:54:29.681682Z","iopub.status.idle":"2025-08-27T05:54:29.713909Z","shell.execute_reply.started":"2025-08-27T05:54:29.681638Z","shell.execute_reply":"2025-08-27T05:54:29.712840Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### [Fusang] Search branch function","metadata":{}},{"cell_type":"code","source":"def search_this_branch(tmp_tree, edge_0, edge_1, current_quartets, current_leave_node_name, queue, dic_for_leave_node_comb_name):\n    modify_tree = get_modify_tree(tmp_tree, edge_0, edge_1, current_leave_node_name)\n    modify_tree.resolve_polytomy(recursive=True)\n    modify_tree.unroot()\n    tmp_tree_score = judge_tree_score(modify_tree, current_quartets, current_leave_node_name, dic_for_leave_node_comb_name)\n\n    dic = {}\n    dic['tree'] = modify_tree\n    dic['score'] = tmp_tree_score\n    queue.put(dic)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.714841Z","iopub.execute_input":"2025-08-27T05:54:29.715169Z","iopub.status.idle":"2025-08-27T05:54:29.740331Z","shell.execute_reply.started":"2025-08-27T05:54:29.715144Z","shell.execute_reply":"2025-08-27T05:54:29.739434Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### [Fusang] Mask Functions","metadata":{}},{"cell_type":"code","source":"def select_mask_node_pair(dl_predict, new_add_taxa):\n    \n    if new_add_taxa <= 9:\n        return None\n\n    mask_node_pair = []\n    \n    current_start = start_end_list[new_add_taxa][0]\n    current_end = start_end_list[new_add_taxa][1]\n    select_distribution = dl_predict[current_start:current_end+1]\n    if np.max(select_distribution) < 0.90:\n        return None\n    else:\n        x,y = nlargest_indices(select_distribution, int(max(10,0.01*len(select_distribution)))) \n        \n    for i in range(0,len(x)):\n        idx = x[i]\n        topology_value = y[i]\n        quartet_comb = comb_of_id[current_start+idx]\n        \n        if topology_value == 0:\n            mask_node_pair.append((quartet_comb[0],quartet_comb[1]))\n        if topology_value == 1:\n            mask_node_pair.append((quartet_comb[0],quartet_comb[2]))\n        if topology_value == 2:\n            mask_node_pair.append((quartet_comb[1],quartet_comb[2]))\n            \n    return mask_node_pair\n\n\ndef mask_edge(tree,node1,node2,edge_list):\n    # mask edge between node1 and node2\n\n    if len(edge_list) <= 3:\n        return edge_list\n\n    ancestor_name = tree.get_common_ancestor(node1,node2).name\n    remove_edge = []\n\n    node = tree.search_nodes(name=node1)[0]\n    while node:\n        if node.name == ancestor_name:\n            break\n\n        edge_0 = node.up.name\n        edge_1 = node.name\n\n        if len(remove_edge) >= len(edge_list) - 3:\n            break\n        remove_edge.append((edge_0, edge_1))\n        node = node.up\n\n    node = tree.search_nodes(name=node2)[0]\n    while node:\n        if node.name == ancestor_name:\n            break\n\n        edge_0 = node.up.name\n        edge_1 = node.name\n\n        if len(remove_edge) >= len(edge_list) - 3:\n            break\n        remove_edge.append((edge_0, edge_1))\n        node = node.up\n\n    for ele in remove_edge:\n        if ele in edge_list:\n            edge_list.remove(ele)\n\n    return edge_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.741481Z","iopub.execute_input":"2025-08-27T05:54:29.741817Z","iopub.status.idle":"2025-08-27T05:54:29.756333Z","shell.execute_reply.started":"2025-08-27T05:54:29.741795Z","shell.execute_reply":"2025-08-27T05:54:29.755470Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### [Fusang] Gen_phylogen_tree\nsearch phylo tree having highest score","metadata":{}},{"cell_type":"code","source":"def gen_phylogenetic_tree(current_quartets, beam_size):\n    '''\n        search the phylogenetic tree having highest score\n        idx: the name of numpy file \n    '''\n    current_leave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n    \n    candidate_tree_beam = []\n\n    quartet_id = leave_node_comb_name[0]\n\n    for _label in [0, 1, 2]:\n        if _label == 0:\n            label_id = \"\".join([quartet_id[0], quartet_id[1], quartet_id[2], quartet_id[3]])\n        elif _label == 1:\n            label_id = \"\".join([quartet_id[0], quartet_id[2], quartet_id[1], quartet_id[3]])\n        elif _label == 2:\n            label_id = \"\".join([quartet_id[0], quartet_id[3], quartet_id[1], quartet_id[2]])\n        \n        _tree = tree_from_quartet(label_id)\n        _tree.unroot()\n\n        _tree_score = current_quartets[0, _label]\n\n        tmp_tree_dict = {'Tree':_tree, 'tree_score':_tree_score}\n        candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n\n    idx_for_internal_node_name_pool = 0\n\n    current_tree_score_beam = []\n    optim_tree_beam = []\n\n    #in the start point set beam size equal to 3\n    for i in range(0, 3):\n        current_tree_score_beam.append(candidate_tree_beam[i]['tree_score'])\n        optim_tree_beam.append(candidate_tree_beam[i]['Tree'])\n    \n    for i in range(4, len(current_leave_node_name)):\n        candidate_tree_beam = []\n\n        for j in range(0, len(optim_tree_beam)):\n            ele = optim_tree_beam[j]\n\n            idx_for_this_iter = 0\n\n            edge_0_list = []\n            edge_1_list = []\n\n            if ele == None:\n                continue\n            optim_tree = ele.copy(\"newick\")\n\n            for node in optim_tree.iter_descendants():\n                tmp_tree = optim_tree.copy(\"newick\")\n                edge_0 = node.up.name\n                edge_1 = node.name\n                if edge_0 == '' or edge_1 == '':\n                    continue\n\n                else:\n                    edge_0_list.append(edge_0)\n                    edge_1_list.append(edge_1)\n\n            queue = multiprocessing.Manager().Queue()\n            para_list = [(tmp_tree, edge_0_list[k], edge_1_list[k], current_quartets, current_leave_node_name[i], queue, dic_for_leave_node_comb_name) for k in range(0, len(edge_0_list))]\n\n            process_num = min(64, 2*i+6)\n            pool = Pool(process_num)\n            pool.starmap(search_this_branch, para_list)\n            pool.close()\n            pool.join()\n\n            candidate_tree_score = []\n            candidate_tree = []\n\n            while not queue.empty():\n                tmp_dic = queue.get()\n                \n                tmp_tree_dict = {'Tree':tmp_dic['tree'], 'tree_score':tmp_dic['score']+current_tree_score_beam[j]}\n                candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n        candidate_tree_beam = candidate_tree_beam[0:beam_size]\n        \n        optim_tree_beam = []\n        current_tree_score_beam = []\n        for ele in candidate_tree_beam:\n            crt_tree = ele['Tree'].copy(\"newick\") \n            for node in crt_tree.traverse(\"preorder\"):\n                if node.name == '':\n                    node.name = str(internal_node_name_pool[idx_for_internal_node_name_pool])\n                    idx_for_internal_node_name_pool += 1\n            optim_tree_beam.append(crt_tree)\n            crt_tree_score = ele['tree_score']\n            current_tree_score_beam.append(crt_tree_score)\n            \n    return optim_tree_beam[0].write(format=9)\n\n\ndef gen_phylogenetic_tree_2(current_quartets, beam_size):\n    '''\n        search the phylogenetic tree having highest score\n        idx: the name of numpy file \n    '''\n    current_leave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n    \n    candidate_tree_beam = []\n\n    quartet_id = leave_node_comb_name[0]\n\n    for _label in [0, 1, 2]:\n        if _label == 0:\n            label_id = \"\".join([quartet_id[0], quartet_id[1], quartet_id[2], quartet_id[3]])\n        elif _label == 1:\n            label_id = \"\".join([quartet_id[0], quartet_id[2], quartet_id[1], quartet_id[3]])\n        elif _label == 2:\n            label_id = \"\".join([quartet_id[0], quartet_id[3], quartet_id[1], quartet_id[2]])\n        \n        _tree = tree_from_quartet(label_id)\n        _tree.unroot()\n\n        _tree_score = current_quartets[0, _label]\n\n        tmp_tree_dict = {'Tree':_tree, 'tree_score':_tree_score}\n        candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n\n    idx_for_internal_node_name_pool = 0\n\n    current_tree_score_beam = []\n    optim_tree_beam = []\n\n    #in the start point set beam size equal to 3\n    for i in range(0, 3):\n        current_tree_score_beam.append(candidate_tree_beam[i]['tree_score'])\n        optim_tree_beam.append(candidate_tree_beam[i]['Tree'])\n    \n    for i in range(4, len(current_leave_node_name)):\n        candidate_tree_beam = []\n\n        for j in range(0, len(optim_tree_beam)):\n            ele = optim_tree_beam[j]\n\n            idx_for_this_iter = 0\n\n            edge_0_list = []\n            edge_1_list = []\n\n            if ele == None:\n                continue\n            optim_tree = ele.copy(\"newick\")\n\n            for node in optim_tree.iter_descendants():\n                tmp_tree = optim_tree.copy(\"newick\")\n                edge_0 = node.up.name\n                edge_1 = node.name\n                if edge_0 == '' or edge_1 == '':\n                    continue\n\n                else:\n                    edge_0_list.append(edge_0)\n                    edge_1_list.append(edge_1)\n\n            edge_list = [(edge_0_list[i], edge_1_list[i]) for i in range(0, len(edge_0_list))]\n            mask_node_pairs = select_mask_node_pair(current_quartets, i)\n\n            if mask_node_pairs != None:\n\n                mask_node_pairs = list(set(mask_node_pairs))\n                for node_pairs in mask_node_pairs:\n                    node1 = chr(ord(u'\\u4e00')+node_pairs[0])\n                    node2 = chr(ord(u'\\u4e00')+node_pairs[1])\n                    \n                    edge_list = mask_edge(ele.copy(\"deepcopy\"),node1,node2,edge_list)\n                    if len(edge_list) <= 3:\n                        break\n\n            edge_0_list = [ele[0] for ele in edge_list]\n            edge_1_list = [ele[1] for ele in edge_list]\n\n            queue = multiprocessing.Manager().Queue()\n            para_list = [(tmp_tree, edge_0_list[k], edge_1_list[k], current_quartets, current_leave_node_name[i], queue, dic_for_leave_node_comb_name) for k in range(0, len(edge_0_list))]\n\n            process_num = min(64, len(edge_0_list))\n            pool = Pool(process_num)\n            pool.starmap(search_this_branch, para_list)\n            pool.close()\n            pool.join()\n\n            candidate_tree_score = []\n            candidate_tree = []\n\n            while not queue.empty():\n                tmp_dic = queue.get()\n                \n                tmp_tree_dict = {'Tree':tmp_dic['tree'], 'tree_score':tmp_dic['score']+current_tree_score_beam[j]}\n                candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n        candidate_tree_beam = candidate_tree_beam[0:beam_size]\n        \n        optim_tree_beam = []\n        current_tree_score_beam = []\n        for ele in candidate_tree_beam:\n            crt_tree = ele['Tree'].copy(\"newick\") \n            for node in crt_tree.traverse(\"preorder\"):\n                if node.name == '':\n                    node.name = str(internal_node_name_pool[idx_for_internal_node_name_pool])\n                    idx_for_internal_node_name_pool += 1\n            optim_tree_beam.append(crt_tree)\n            crt_tree_score = ele['tree_score']\n            current_tree_score_beam.append(crt_tree_score)\n\n    return optim_tree_beam[0].write(format=9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.759965Z","iopub.execute_input":"2025-08-27T05:54:29.760673Z","iopub.status.idle":"2025-08-27T05:54:29.791300Z","shell.execute_reply.started":"2025-08-27T05:54:29.760644Z","shell.execute_reply":"2025-08-27T05:54:29.790097Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### [Fusang] Util functions\n- transform_str\n- cmp\n- get_numpy","metadata":{}},{"cell_type":"code","source":"def transform_str(str_a, taxa_name):\n    str_b = ''\n    id_set = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n\n    for i in range(0, len(str_a)):\n        if str_a[i] in id_set:\n            str_b += taxa_name[ord(str_a[i])-ord(u'\\u4e00')]\n        else:\n            str_b += str_a[i]\n\n    return str_b\n\n\ndef cmp(a, b):\n    if int(b) > int(a):\n        return -1\n    if int(a) < int(b):\n        return 1\n    return 0\n\n\ndef get_numpy(aln_file):\n    '''\n        current version only supports the total length of msa less than 10K\n    '''\n    aln = open(aln_file)\n    dic = {'A':'0','T':'1','C':'2','G':'3','-':'4', 'N':'4'}\n\n    # for masking other unknown bases\n    other_base = ['R', 'Y', 'K', 'M', 'U', 'S', 'W', 'B', 'D', 'H', 'V', 'X']\n    for ele in other_base:\n        dic[ele] = '4'\n\n    matrix_out=[]\n    fasta_dic={}\n    for line in aln:\n        if line[0]==\">\":\n            header=line[1:].rstrip('\\n').strip()\n            fasta_dic[header]=[]\n        elif line[0].isalpha() or line[0]=='-':\n            for base, num in dic.items():\n                line=line[:].rstrip('\\n').strip().replace(base,num)\n            line=list(line)\n            line=[int(n) for n in line]\n            fasta_dic[header] += line+[4]*(14000-len(line))\n\n    taxa_block=[]\n    for taxa in sorted(list(fasta_dic.keys()), key=functools.cmp_to_key(cmp)):\n        taxa_block.append(fasta_dic[taxa.strip()])\n    fasta_dic={}\n    matrix_out.append(taxa_block)\n\n    return np.array(matrix_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.792258Z","iopub.execute_input":"2025-08-27T05:54:29.792652Z","iopub.status.idle":"2025-08-27T05:54:29.818117Z","shell.execute_reply.started":"2025-08-27T05:54:29.792625Z","shell.execute_reply":"2025-08-27T05:54:29.817116Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### [QPhyloformer] Defining QP models for different lenghts (>1200 or >240)","metadata":{}},{"cell_type":"code","source":"class RowAttention(nn.Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n\n    def forward(self, x):\n        # x: (B, N=4, L, D)\n        B, N, L, D = x.shape\n        x = x.transpose(1, 2)         # (B, L, N, D)\n        x = x.reshape(B * L, N, D)\n        x, _ = self.attn(x, x, x)\n        x = x.reshape(B, L, N, D).transpose(1, 2)  # (B, N, L, D)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.818771Z","iopub.execute_input":"2025-08-27T05:54:29.819063Z","iopub.status.idle":"2025-08-27T05:54:29.834693Z","shell.execute_reply.started":"2025-08-27T05:54:29.819042Z","shell.execute_reply":"2025-08-27T05:54:29.833684Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ColumnAttention(nn.Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n\n    def forward(self, x):\n        # x: (B, N=4, L, D)\n        B, N, L, D = x.shape\n        x = x.reshape(B * N, L, D)\n        x, _ = self.attn(x, x, x)\n        x = x.reshape(B, N, L, D)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.835831Z","iopub.execute_input":"2025-08-27T05:54:29.836439Z","iopub.status.idle":"2025-08-27T05:54:29.859003Z","shell.execute_reply.started":"2025-08-27T05:54:29.836407Z","shell.execute_reply":"2025-08-27T05:54:29.858114Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MSABlock(nn.Module):\n    def __init__(self, dim, heads, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n\n        self.row_attn = RowAttention(dim, heads)\n        self.col_attn = ColumnAttention(dim, heads)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim), # \n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        # Row Attention + residual\n        x = x + self.row_attn(self.norm1(x))\n        # Column Attention + residual\n        x = x + self.col_attn(self.norm2(x))\n        # Feed‑Forward + residual\n        x = x + self.ffn(self.norm3(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.859805Z","iopub.execute_input":"2025-08-27T05:54:29.860079Z","iopub.status.idle":"2025-08-27T05:54:29.885730Z","shell.execute_reply.started":"2025-08-27T05:54:29.860059Z","shell.execute_reply":"2025-08-27T05:54:29.884909Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class QPhyloformer(nn.Module):\n    def __init__(self, dim=64, heads=4, layers_num=4):\n        super().__init__()\n        self.embedding = nn.Embedding(\n            6, # A(0),T(1),C(2),G(3),-(4), N(4), CLS(5)\n            dim\n        )\n        self.taxon_embed = nn.Embedding(4, dim)  # 4 fixed taxa\n        self.blocks = nn.ModuleList([\n            MSABlock(dim, heads) for _ in range(layers_num)\n        ])\n        self.norm = nn.LayerNorm(dim)\n\n        # (B, D, 4, L)\n        self.classifier = nn.Sequential(\n            # nn.AdaptiveAvgPool2d((1, 1)),  # pool over taxa × sites -> (B, D, 1, 1)\n            # nn.Flatten(), # (B, D)\n            nn.Linear(dim, 3),  # (B, 3)\n            nn.Softmax(dim=1)\n            # nn.Flatten(),               # (B, D, 4, L) -> (B, D*4*L)\n            # nn.Linear(dim * 4 * seq_len, 3), # fixed input size\n            # nn.Softmax(dim=1)\n\n            # nn.Linear(4*dim, hidden_dim),\n            # nn.ReLU(),\n            # nn.Linear(4*dim, 3),\n            # nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        # x: (B, 4, L), each entry ∈ [0, VOCAB_SIZE-1]\n        if isinstance(x, np.ndarray):\n            x = torch.from_numpy(x)\n        x = x.long()\n        B, N, L = x.shape\n\n        # Prepend CLS token (integer 5)\n        # cls_tokens = torch.full((B, 4, 1), 5, dtype=torch.long, device=x.device)  # shape: (B, 4, 1)\n        # x = torch.cat([cls_tokens, x], dim=2)  # (B, N, L+1)\n        \n        x = self.embedding(x)  # (B, 4, L, D)\n        taxon_ids = torch.arange(N, device=x.device).unsqueeze(0).expand(B, N)\n        taxon_embed = self.taxon_embed(taxon_ids).unsqueeze(2)  # (B, 4, 1, D)\n        x = x + taxon_embed  # (B, 4, L, D)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.norm(x) # (B, 4, L, D)\n        B, N, L, D = x.shape\n        # cls_embedding = x[:, :, 0, :]  # (B, 4, D)\n        # x = cls_embedding.mean(dim=1)   # pool over taxa -> (B, D)\n\n        x = x.mean(dim=(1,2)) # (B, D)\n        # x = cls_embedding.reshape(B, 4 * D) # (B, 4 * D)\n        prediction = self.classifier(x)  # (B, 3)\n        return prediction\n\n    def load_weights(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n        self.to(torch.device('cpu'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.886804Z","iopub.execute_input":"2025-08-27T05:54:29.887142Z","iopub.status.idle":"2025-08-27T05:54:29.909676Z","shell.execute_reply.started":"2025-08-27T05:54:29.887113Z","shell.execute_reply":"2025-08-27T05:54:29.908644Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_qp_model_240():\n    return QPhyloformer(dim=64, heads=4, layers_num=4)\n\ndef get_qp_model_1200():\n    return QPhyloformer(dim=128, heads=8, layers_num=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.910699Z","iopub.execute_input":"2025-08-27T05:54:29.911058Z","iopub.status.idle":"2025-08-27T05:54:29.942979Z","shell.execute_reply.started":"2025-08-27T05:54:29.911027Z","shell.execute_reply":"2025-08-27T05:54:29.941824Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def get_dl_model_1200():\n    '''\n    get the definition of dl model 1200\n    this model aims to solve the default case \n    which are length larger than 1200 \n    '''\n    conv_x=[4,1,1,1,1,1,1,1]\n    conv_y=[1,2,2,2,2,2,2,2]\n    pool=[1,4,4,4,2,2,2,1]\n    filter_s=[1024,1024,128,128,128,128,128,128]\n\n    visible = layers.Input(shape=(4,1200,1))\n    x = visible\n        \n    for l in list(range(0,8)):\n        x = layers.ZeroPadding2D(padding=((0, 0), (0,conv_y[l]-1)))(x)        \n        x = layers.Conv2D(filters=filter_s[l], kernel_size=(conv_x[l], conv_y[l]), strides=1, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(rate=0.2)(x)\n        x = layers.AveragePooling2D(pool_size=(1,pool[l]))(x)\n        \n    flat = layers.Flatten()(x)\n\n    y = tf.keras.layers.Reshape((4,1200))(visible)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(y)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(y)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(y)\n    flat = tf.keras.layers.concatenate([flat, y],axis=-1)\n\n    hidden1 = layers.Dense(1024,activation='relu')(flat)\n    drop1 = layers.Dropout(rate=0.2)(hidden1)\n    output = layers.Dense(3, activation='softmax')(drop1)\n    model = tf.keras.Model(inputs=visible, outputs=output)\n\n    return model\n\n\ndef get_dl_model_240():\n    '''\n    get the definition of dl model 240\n    this model aims to solve the short length case \n    which are length larger than 240 \n    '''\n    conv_x=[4,1,1,1,1,1,1,1]\n    conv_y=[1,2,2,2,2,2,2,2]\n    pool=[1,2,2,2,2,2,2,2]\n    filter_s=[1024,1024,128,128,128,128,128,128]\n\n    visible = layers.Input(shape=(4,240,1))\n    x = visible\n\n    for l in list(range(0,8)):\n        x = layers.ZeroPadding2D(padding=((0, 0), (0,conv_y[l]-1)))(x)        \n        x = layers.Conv2D(filters=filter_s[l], kernel_size=(conv_x[l], conv_y[l]), strides=1, activation='relu')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(rate=0.2)(x)\n        x = layers.AveragePooling2D(pool_size=(1,pool[l]))(x)\n        #print(x.shape)\n\n    flat = layers.Flatten()(x)\n\n    y = tf.keras.layers.Reshape((4,240))(visible)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(y)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(y)\n    y = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(y)\n    flat = tf.keras.layers.concatenate([flat, y],axis=-1)\n\n    hidden1 = layers.Dense(1024,activation='relu')(flat)\n    drop1 = layers.Dropout(rate=0.2)(hidden1)\n    output = layers.Dense(3, activation='softmax')(drop1)\n    model = tf.keras.Model(inputs=visible, outputs=output)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.944054Z","iopub.execute_input":"2025-08-27T05:54:29.944395Z","iopub.status.idle":"2025-08-27T05:54:29.968789Z","shell.execute_reply.started":"2025-08-27T05:54:29.944368Z","shell.execute_reply":"2025-08-27T05:54:29.967843Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### [Fusang] Predict Functions (slide_window)","metadata":{}},{"cell_type":"code","source":"def fill_dl_predict_each_slide_window(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 1200))\n\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n        test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n        predicted = dl_model.predict(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            dl_predict[i*50000+j,:] += predicted[j,:]\n        #dl_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 1200))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n    predicted = dl_model.predict(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        dl_predict[iters*50000+j,:] += predicted[j,:]\n    #dl_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]\n\ndef fill_dl_predict_each_slide_window_2(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 240))\n\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n        test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n        predicted = dl_model.predict(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            dl_predict[i*50000+j,:] += predicted[j,:]\n        #dl_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 240))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n    predicted = dl_model.predict(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        dl_predict[iters*50000+j,:] += predicted[j,:]\n    #dl_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.969993Z","iopub.execute_input":"2025-08-27T05:54:29.970364Z","iopub.status.idle":"2025-08-27T05:54:29.997424Z","shell.execute_reply.started":"2025-08-27T05:54:29.970338Z","shell.execute_reply":"2025-08-27T05:54:29.996205Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def fill_qp_predict_each_slide_window(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 1200))\n\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n        test_seq = torch.from_numpy(batch_seq).long()\n        predicted = qp_model(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            qp_predict[i*50000+j,:] += predicted[j,:]\n        #qp_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 1200))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = torch.from_numpy(batch_seq).long()\n    predicted = qp_model(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        qp_predict[iters*50000+j,:] += predicted[j,:]\n    #qp_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]\n    \ndef fill_qp_predict_each_slide_window_2(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 240))\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n        \n        # test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n        test_seq = torch.from_numpy(batch_seq).long()  # shape: (50000, 4, 240)\n        predicted = qp_model(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            qp_predict[i*50000+j,:] += predicted[j,:]\n        #qp_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 240))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = torch.from_numpy(batch_seq).long()\n    predicted = qp_model(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        qp_predict[iters*50000+j,:] += predicted[j,:]\n    #qp_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:29.998342Z","iopub.execute_input":"2025-08-27T05:54:29.998645Z","iopub.status.idle":"2025-08-27T05:54:30.030983Z","shell.execute_reply.started":"2025-08-27T05:54:29.998623Z","shell.execute_reply":"2025-08-27T05:54:30.029929Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def fill_dl_predict(window_number):\n    step = (len_of_msa - 1200) // window_number\n    start_idx = 0\n    for i in range(0, window_number):\n        end_idx = start_idx + 1200\n        fill_dl_predict_each_slide_window(start_idx, end_idx)\n        start_idx += step\n           \ndef fill_dl_predict_2(window_number):\n    if len_of_msa > 240:\n        step = (len_of_msa - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_dl_predict_each_slide_window_2(start_idx, end_idx)\n            start_idx += step\n\n    else:\n        step = (250 - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_dl_predict_each_slide_window_2(start_idx, end_idx)\n            start_idx += step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.032004Z","iopub.execute_input":"2025-08-27T05:54:30.032277Z","iopub.status.idle":"2025-08-27T05:54:30.062391Z","shell.execute_reply.started":"2025-08-27T05:54:30.032252Z","shell.execute_reply":"2025-08-27T05:54:30.060993Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def fill_qp_predict(window_number):\n    step = (len_of_msa - 1200) // window_number\n    start_idx = 0\n    for i in range(0, window_number):\n        end_idx = start_idx + 1200\n        fill_qp_predict_each_slide_window(start_idx, end_idx)\n        start_idx += step\n        \ndef fill_qp_predict_2(window_number):\n    if len_of_msa > 240:\n        step = (len_of_msa - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_qp_predict_each_slide_window_2(start_idx, end_idx)\n            start_idx += step\n\n    else:\n        step = (250 - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_qp_predict_each_slide_window_2(start_idx, end_idx)\n\n            start_idx += step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.063452Z","iopub.execute_input":"2025-08-27T05:54:30.063731Z","iopub.status.idle":"2025-08-27T05:54:30.086165Z","shell.execute_reply.started":"2025-08-27T05:54:30.063710Z","shell.execute_reply":"2025-08-27T05:54:30.085143Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### [QPhyloformer] Training","metadata":{}},{"cell_type":"markdown","source":"#### Hyperparametes","metadata":{}},{"cell_type":"code","source":"data_type = 'S1G'\ndata_dir = f'/kaggle/input/fusang-10000/{data_type}/{{split}}'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.087046Z","iopub.execute_input":"2025-08-27T05:54:30.087331Z","iopub.status.idle":"2025-08-27T05:54:30.109474Z","shell.execute_reply.started":"2025-08-27T05:54:30.087305Z","shell.execute_reply":"2025-08-27T05:54:30.108178Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"NUM_EPOCHS      = 60\nLEARNING_RATE   = 1e-4\nWEIGHT_DECAY    = 1e-5\nBATCH_SIZE      = 32\n\n# Scheduler (ReduceLROnPlateau)\nLR_FACTOR       = 0.7   # multiply LR by this factor\nLR_PATIENCE     = 3     # epochs with no improvement before reducing LR\n\n# Regularization\nLABEL_SMOOTHING = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.110567Z","iopub.execute_input":"2025-08-27T05:54:30.110838Z","iopub.status.idle":"2025-08-27T05:54:30.129640Z","shell.execute_reply.started":"2025-08-27T05:54:30.110818Z","shell.execute_reply":"2025-08-27T05:54:30.128446Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def load_data(split):\n    msa_folder = data_dir.format(split=split) + \"/seq\"\n    label_folder = data_dir.format(split=split) + \"/label\"\n    quartet_data = []\n\n    msa_files = sorted(os.listdir(msa_folder))\n    label_files = sorted(os.listdir(label_folder))\n\n    print(f\"[Data] Found {len(msa_files)} MSA files and {len(label_files)} label files\")\n\n    # wrap with tqdm\n    for seq_file, label_file in tqdm(zip(msa_files, label_files),\n                                     total=min(len(msa_files), len(label_files)),\n                                     desc=f\"Loading {split} data\"):\n        label_path = os.path.join(label_folder, label_file)\n        label = np.load(label_path, allow_pickle=True)\n\n        seq_path = os.path.join(msa_folder, seq_file)\n        seq = np.load(seq_path, allow_pickle=True)\n\n        quartet_data.append((seq, label))\n\n    return quartet_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.130747Z","iopub.execute_input":"2025-08-27T05:54:30.131043Z","iopub.status.idle":"2025-08-27T05:54:30.156215Z","shell.execute_reply.started":"2025-08-27T05:54:30.131022Z","shell.execute_reply":"2025-08-27T05:54:30.154973Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# quartet_data = load_data(\n#     '/kaggle/input/fusang/S1G/numpy_file/seq', \n#     '/kaggle/input/fusang/S1G/numpy_file/label'\n# )\n\n# total_size = len(quartet_data)\n# train_size = int(0.7 * total_size)\n# val_size   = int(0.15 * total_size)\n# test_size  = total_size - train_size - val_size\n\n# train_data, val_data, test_data = random_split(\n#     quartet_data, [train_size, val_size, test_size],\n#     generator=torch.Generator().manual_seed(42)\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.157553Z","iopub.execute_input":"2025-08-27T05:54:30.157915Z","iopub.status.idle":"2025-08-27T05:54:30.182820Z","shell.execute_reply.started":"2025-08-27T05:54:30.157854Z","shell.execute_reply":"2025-08-27T05:54:30.181466Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# import os\n# import shutil\n# import zipfile\n# from torch.utils.data import Subset\n\n# # Output directory\n# output_dir = \"dataset_split\"\n# os.makedirs(output_dir, exist_ok=True)\n\n# # Helper: save split into folder\n# def save_split(split_name, split_data):\n#     split_dir = os.path.join(output_dir, split_name)\n#     seq_dir = os.path.join(split_dir, \"seq\")\n#     label_dir = os.path.join(split_dir, \"label\")\n#     os.makedirs(seq_dir, exist_ok=True)\n#     os.makedirs(label_dir, exist_ok=True)\n\n#     for i, (seq, label) in enumerate(split_data):\n#         # Save seq\n#         seq_path = os.path.join(seq_dir, f\"{i}.npy\")\n#         np.save(seq_path, seq)\n\n#         # Save label\n#         label_path = os.path.join(label_dir, f\"{i}.npy\")\n#         np.save(label_path, label)\n\n# # Save train/valid/test\n# save_split(\"train\", Subset(quartet_data, train_data.indices))\n# save_split(\"valid\", Subset(quartet_data, val_data.indices))\n# save_split(\"test\", Subset(quartet_data, test_data.indices))\n\n# # Make zip file\n# zip_path = \"dataset.zip\"\n# shutil.make_archive(\"dataset\", 'zip', output_dir)\n\n# print(f\"✅ Dataset saved to {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.184056Z","iopub.execute_input":"2025-08-27T05:54:30.184341Z","iopub.status.idle":"2025-08-27T05:54:30.207137Z","shell.execute_reply.started":"2025-08-27T05:54:30.184320Z","shell.execute_reply":"2025-08-27T05:54:30.205977Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_data = load_data(\"train\")\nval_data = load_data(\"valid\")\ntest_data =  load_data(\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:54:30.208188Z","iopub.execute_input":"2025-08-27T05:54:30.208459Z","iopub.status.idle":"2025-08-27T05:56:12.133794Z","shell.execute_reply.started":"2025-08-27T05:54:30.208438Z","shell.execute_reply":"2025-08-27T05:56:12.132682Z"}},"outputs":[{"name":"stdout","text":"[Data] Found 7000 MSA files and 7000 label files\n","output_type":"stream"},{"name":"stderr","text":"Loading train data: 100%|██████████| 7000/7000 [01:09<00:00, 100.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Data] Found 1500 MSA files and 1500 label files\n","output_type":"stream"},{"name":"stderr","text":"Loading valid data: 100%|██████████| 1500/1500 [00:15<00:00, 94.42it/s] \n","output_type":"stream"},{"name":"stdout","text":"[Data] Found 1500 MSA files and 1500 label files\n","output_type":"stream"},{"name":"stderr","text":"Loading test data: 100%|██████████| 1500/1500 [00:16<00:00, 92.08it/s] \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE)\n\nif data_type == 'S2G':\n    model = get_qp_model_1200()\nelif data_type == 'S1G':\n    model = get_qp_model_240()\nelse:\n    print(\"Invalid Data type\")\n    \nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.993279Z","iopub.status.idle":"2025-08-27T05:56:33.993722Z","shell.execute_reply.started":"2025-08-27T05:56:33.993493Z","shell.execute_reply":"2025-08-27T05:56:33.993512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss, correct = 0, 0\n\n    for x, y in tqdm(dataloader, desc=\"Training\", unit=\"batch\"):\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (logits.argmax(dim=1) == y).sum().item()\n\n    return total_loss / len(dataloader.dataset), correct / len(dataloader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.994657Z","iopub.status.idle":"2025-08-27T05:56:33.995071Z","shell.execute_reply.started":"2025-08-27T05:56:33.994839Z","shell.execute_reply":"2025-08-27T05:56:33.994857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val_one_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, correct = 0, 0\n    \n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            \n            total_loss += loss.item() * x.size(0)\n            correct += (logits.argmax(dim=1) == y).sum().item()\n\n    return total_loss / len(val_loader.dataset), correct / len(val_loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.996868Z","iopub.status.idle":"2025-08-27T05:56:33.997214Z","shell.execute_reply.started":"2025-08-27T05:56:33.997063Z","shell.execute_reply":"2025-08-27T05:56:33.997077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=LR_FACTOR, patience=LR_PATIENCE, verbose=True\n)\n\nhistory = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    # --- training loop ---\n    # model.train()\n    # total_loss, correct = 0, 0\n    # for x, y in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n    #     x, y = x.to(device), y.to(device)\n    #     optimizer.zero_grad()\n    #     logits = model(x)\n    #     loss = criterion(logits, y)\n    #     loss.backward()\n    #     optimizer.step()\n    #     total_loss += loss.item() * x.size(0)\n    #     correct += (logits.argmax(dim=1) == y).sum().item()\n\n    # train_loss = total_loss / len(train_loader.dataset)\n    # train_acc = correct / len(train_loader.dataset)\n\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n\n    # --- validation loop ---\n    # model.eval()\n    # total_loss, correct = 0, 0\n    # with torch.no_grad():\n    #     for x, y in val_loader:\n    #         x, y = x.to(device), y.to(device)\n    #         logits = model(x)\n    #         loss = criterion(logits, y)\n    #         total_loss += loss.item() * x.size(0)\n    #         correct += (logits.argmax(dim=1) == y).sum().item()\n\n    # val_loss = total_loss / len(val_loader.dataset)\n    # val_acc = correct / len(val_loader.dataset)\n    val_loss, val_acc = val_one_epoch(model, dataloader, criterion, device)\n\n    scheduler.step(val_loss)\n    \n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {epoch}: \"\n          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f} | \"\n          f\"LR={current_lr:.6f}\")\n\n# --- test evaluation + confusion matrix ---\nmodel.eval()\ntotal_loss, correct = 0, 0\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for x, y in test_loader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        total_loss += loss.item() * x.size(0)\n        correct += (logits.argmax(dim=1) == y).sum().item()\n\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(y.cpu().numpy())\n\ntest_loss = total_loss / len(test_loader.dataset)\ntest_acc = accuracy_score(all_labels, all_preds)\ntest_f1 = f1_score(all_labels, all_preds, average='weighted')\nprint(f\"\\nFinal Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}\")\n\n# --- confusion matrix ---\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.show()\n\n# --- plots for training curves ---\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(1,2,2)\nplt.plot(history[\"train_acc\"], label=\"Train Acc\")\nplt.plot(history[\"val_acc\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:12.262677Z","iopub.execute_input":"2025-08-27T05:56:12.263108Z","iopub.status.idle":"2025-08-27T05:56:33.961701Z","shell.execute_reply.started":"2025-08-27T05:56:12.263078Z","shell.execute_reply":"2025-08-27T05:56:33.960100Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\nTraining:   2%|▏         | 5/219 [00:17<12:38,  3.54s/batch]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2030104980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# train_acc = correct / len(train_loader.dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# --- validation loop ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/4152001944.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Plot training and validation loss\nplt.plot(history[\"train_loss\"], label='Training Loss')\nplt.plot(history[\"val_loss\"], label='Validation Loss')\nplt.xlabel('Epoch')\n# plt.ylim(0, 2)\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.962873Z","iopub.status.idle":"2025-08-27T05:56:33.963324Z","shell.execute_reply.started":"2025-08-27T05:56:33.963101Z","shell.execute_reply":"2025-08-27T05:56:33.963117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training accuracy\n# Plot training and validation accuracy\nplt.plot(history[\"train_acc\"], label='Training Accuracy')\nplt.plot(history[\"val_acc\"], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.964586Z","iopub.status.idle":"2025-08-27T05:56:33.965054Z","shell.execute_reply.started":"2025-08-27T05:56:33.964812Z","shell.execute_reply":"2025-08-27T05:56:33.964830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"checkpoint.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.965953Z","iopub.status.idle":"2025-08-27T05:56:33.966255Z","shell.execute_reply.started":"2025-08-27T05:56:33.966123Z","shell.execute_reply":"2025-08-27T05:56:33.966135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Training","metadata":{}},{"cell_type":"code","source":"if data_type == 'S1G':\n    dl_model = get_dl_model_240()\nelif data_type == 'S2G':\n    dl_model = get_dl_model_1200()\nelse:\n    print(\"Invalid data type\")\n\n# Calculate total parameters and approximate size in MB\ntotal_params = dl_model.count_params()\nsize_MB = total_params * 4 / (1024**2)  # float32\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Approximate model size: {size_MB:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.968086Z","iopub.status.idle":"2025-08-27T05:56:33.968466Z","shell.execute_reply.started":"2025-08-27T05:56:33.968262Z","shell.execute_reply":"2025-08-27T05:56:33.968279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dl_model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n    metrics=['accuracy']\n)\n\ntrain_indices = train_data.indices\nX_train = np.array([quartet_data[i][0] for i in train_indices])\ny_train = np.array([quartet_data[i][1] for i in train_indices])\nX_train = np.expand_dims(X_train, axis=-1)\ny_train = tf.one_hot(y_train, depth=3)\n\nval_indices = val_data.indices\nX_val = np.array([quartet_data[i][0] for i in val_indices])\ny_val = np.array([quartet_data[i][1] for i in val_indices])\nX_val = np.expand_dims(X_val, axis=-1)\ny_val = tf.one_hot(y_val, depth=3)\n\nhistory = dl_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.969805Z","iopub.status.idle":"2025-08-27T05:56:33.970185Z","shell.execute_reply.started":"2025-08-27T05:56:33.970009Z","shell.execute_reply":"2025-08-27T05:56:33.970023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(history.history[\"loss\"], label=\"Train Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(1,2,2)\nplt.plot(history.history[\"accuracy\"], label=\"Train Acc\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.971705Z","iopub.status.idle":"2025-08-27T05:56:33.972096Z","shell.execute_reply.started":"2025-08-27T05:56:33.971901Z","shell.execute_reply":"2025-08-27T05:56:33.971920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plot training and validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylim(0, 2)\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.973251Z","iopub.status.idle":"2025-08-27T05:56:33.973750Z","shell.execute_reply.started":"2025-08-27T05:56:33.973547Z","shell.execute_reply":"2025-08-27T05:56:33.973566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training accuracy\n# Plot training and validation accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.975343Z","iopub.status.idle":"2025-08-27T05:56:33.975759Z","shell.execute_reply.started":"2025-08-27T05:56:33.975557Z","shell.execute_reply":"2025-08-27T05:56:33.975578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_indices = test_data.indices\nX_test = np.array([quartet_data[i][0] for i in test_indices])\ny_test = np.array([quartet_data[i][1] for i in test_indices])\nX_test = np.expand_dims(X_test, axis=-1)\ny_test = tf.one_hot(y_test, depth=3)\n\npredictions = dl_model.predict(X_test)\nprint(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.976875Z","iopub.status.idle":"2025-08-27T05:56:33.977247Z","shell.execute_reply.started":"2025-08-27T05:56:33.977079Z","shell.execute_reply":"2025-08-27T05:56:33.977102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_labels = np.argmax(predictions, axis=1)\nprint(predicted_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.978771Z","iopub.status.idle":"2025-08-27T05:56:33.979509Z","shell.execute_reply.started":"2025-08-27T05:56:33.979225Z","shell.execute_reply":"2025-08-27T05:56:33.979246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_labels = np.argmax(y_test, axis=1)\nprint(true_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.980924Z","iopub.status.idle":"2025-08-27T05:56:33.981265Z","shell.execute_reply.started":"2025-08-27T05:56:33.981126Z","shell.execute_reply":"2025-08-27T05:56:33.981140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\naccuracy = accuracy_score(true_labels, predicted_labels)\n\n# Calculate F1 score\nf1 = f1_score(true_labels, predicted_labels, average='weighted')\n\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.982068Z","iopub.status.idle":"2025-08-27T05:56:33.982344Z","shell.execute_reply.started":"2025-08-27T05:56:33.982207Z","shell.execute_reply":"2025-08-27T05:56:33.982219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- confusion matrix ---\ncm = confusion_matrix(true_labels, predicted_labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.983942Z","iopub.status.idle":"2025-08-27T05:56:33.984360Z","shell.execute_reply.started":"2025-08-27T05:56:33.984145Z","shell.execute_reply":"2025-08-27T05:56:33.984163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dl_model.save_weights('checkpoint.weights.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.985799Z","iopub.status.idle":"2025-08-27T05:56:33.986227Z","shell.execute_reply.started":"2025-08-27T05:56:33.986026Z","shell.execute_reply":"2025-08-27T05:56:33.986043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"msa_dir = \"/kaggle/input/sample-msa/a.fas\"\nsave_prefix = \"tree_for_a\"\nbeam_size = '1'\nsequence_type = 'standard'\nbranch_model = 'gamma'\nwindow_coverage = '1'\nverbose = True\n\n\ndef vprint(*a, **kw):\n    if verbose:\n        print(*a, **kw)\n\nvprint(\"[QPhyloformer] Starting workflow with arguments:\")\nvprint(f\"  msa_dir: {msa_dir}\")\nvprint(f\"  save_prefix: {save_prefix}\")\nvprint(f\"  beam_size: {beam_size}\")\nvprint(f\"  sequence_type: {sequence_type}\")\nvprint(f\"  branch_model: {branch_model}\")\nvprint(f\"  window_coverage: {window_coverage}\")\n\nflag = 0\nsupport_format = ['.fas', '.phy', '.fasta', 'phylip']\nbio_format = ['fasta', 'phylip', 'fasta', 'phylip']\n\ntaxa_name = {}\n\nfor i in range(0, len(support_format)):\n    ele = support_format[i]\n    if msa_dir.endswith(ele):\n        flag = 1\n        try:\n            vprint(f\"[QPhyloformer] Reading alignment file: {msa_dir} as {bio_format[i]}\")\n            alignment = AlignIO.read(open(msa_dir), bio_format[i])\n\n            len_of_msa = len(alignment[0].seq)\n            taxa_num = len(alignment)\n            vprint(f\"[QPhyloformer] Alignment loaded. Number of taxa: {taxa_num}, MSA length: {len_of_msa}\")\n\n            save_alignment = save_prefix + '_fusang.fas'\n            with open(save_alignment,'w') as f:\n                for record in alignment:\n                    taxa_name[len(taxa_name)] = record.id\n                    f.write('>'+str(len(taxa_name)-1)+'\\n')\n                    f.write(str(record.seq)+'\\n')\n            vprint(f\"[QPhyloformer] Alignment saved to: {save_alignment}\")\n        except Exception as e:\n            print('Something wrong about your msa file, please check your msa file')\n            if verbose:\n                print(f\"[QPhyloformer] Exception: {e}\")\n        break\n\nif flag == 0:\n    print('we do not support this format of msa')\n    sys.exit(1)\n\nstart_end_list = [None, None, None]\n\nend = -1\nfor i in range(3, 100):\n    start = end + 1\n    end = start + int(comb_math(i,3)) - 1\n    start_end_list.append((start,end))\n\nid_for_taxa = [i for i in range(0, taxa_num)]\ncomb_of_id = list(combinations(id_for_taxa, 4))\ncomb_of_id.sort(key=lambda ele: ele[-1])\n\nleave_node_id = [i for i in range(0, taxa_num)]\nleave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\nleave_node_comb_id = comb_of_id\n\nleave_node_comb_name = []\n\ndic_for_leave_node_comb_name = {}\n\nfor ele in leave_node_comb_id:\n    term = [chr(ord(u'\\u4e00')+id) for id in ele]\n    dic_for_leave_node_comb_name[\"\".join(term)] = len(dic_for_leave_node_comb_name)\n    leave_node_comb_name.append(\"\".join(term))\n\ninternal_node_name_pool = ['internal_node_' + str(i) for i in range(3, 3000)]\n\nfusang_msa_dir = save_prefix + '_fusang.fas'\nvprint(f\"[QPhyloformer] Converting alignment to numpy array...\")\norg_seq = get_numpy(fusang_msa_dir)\nos.remove(fusang_msa_dir)\nvprint(f\"[QPhyloformer] Alignment numpy array created and temp file removed.\")\n\nwindow_number = 1\n\nif len_of_msa <= 1210:\n    qp_model = get_qp_model()\n    if sequence_type == 'standard' and branch_model == 'gamma':\n        vprint(\"[QPhyloformer] Loading weights: /kaggle/working/S1G.pt\")\n        qp_model.load_weights(filepath='/kaggle/working/S1G.pt')\n    window_number = int(len_of_msa * float(window_coverage) // 240 + 1) \n    vprint(f\"[QPhyloformer] Window number set to {window_number}\")\nelse:\n    qp_model = get_qp_model()\n    if sequence_type == 'standard' and branch_model == 'gamma':\n        vprint(\"[QPhyloformer] Loading weights: /kaggle/working/S2G.pt\")\n        qp_model.load_weights(filepath='/kaggle/working/S2G.pt')\n    window_number = int(len_of_msa * float(window_coverage) // 1200 + 1) \n    vprint(f\"[QPhyloformer] Window number set to {window_number}\")\n\nvprint(f\"[QPhyloformer] Filling QP predictions...\")\n\nqp_predict = np.zeros((len(comb_of_id), 3))\nif len_of_msa <= 1210:\n    fill_qp_predict(window_number)\nelse:\n    fill_qp_predict_2(window_number)\n    \nqp_predict /= window_number\n\nvprint(f\"[QPhyloformer] QP predictions normalized by window number.\")\n\nif not os.path.exists('./qp_output/'):\n    vprint(f\"[QPhyloformer] Creating output directory ./dl_output/\")\n    os.mkdir('./qp_output/')\n\nvprint(f\"[QPhyloformer] Searching for best phylogenetic tree...\")\n\nqp_predict_np = qp_predict.detach().cpu().numpy()\nif taxa_num > 10:\n    searched_tree = transform_str(gen_phylogenetic_tree_2(qp_predict_np, int(beam_size)), taxa_name)\nelse:\n    searched_tree = transform_str(gen_phylogenetic_tree(qp_predict_np, int(beam_size)), taxa_name)\nvprint(f\"[QPhyloformer] Tree search complete.\")\nvprint(f\"[QPhyloformer] Tree: {searched_tree}\")\nbuild_log = open('./qp_output/{}.txt'.format(save_prefix), 'a')\nbuild_log.write(searched_tree)\nbuild_log.close()\nvprint(f\"[QPhyloformer] Tree written to ./qp_output/{save_prefix}.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.987234Z","iopub.status.idle":"2025-08-27T05:56:33.987517Z","shell.execute_reply.started":"2025-08-27T05:56:33.987393Z","shell.execute_reply":"2025-08-27T05:56:33.987404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from Bio import Phylo\nfrom io import StringIO\n\n# Parse the Newick string\ntree = Phylo.read(StringIO(searched_tree), \"newick\")\n\n# Draw tree in ASCII\nPhylo.draw_ascii(tree)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.989108Z","iopub.status.idle":"2025-08-27T05:56:33.989467Z","shell.execute_reply.started":"2025-08-27T05:56:33.989282Z","shell.execute_reply":"2025-08-27T05:56:33.989296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"msa_dir = \"/kaggle/input/sample-msa/a.fas\"\nsave_prefix = \"tree_for_a\"\nbeam_size = '1'\nsequence_type = 'standard'\nbranch_model = 'gamma'\nwindow_coverage = '1'\nverbose = True\n\ndef vprint(*a, **kw):\n    if verbose:\n        print(*a, **kw)\n\nvprint(\"[Fusang] Starting workflow with arguments:\")\nvprint(f\"  msa_dir: {msa_dir}\")\nvprint(f\"  save_prefix: {save_prefix}\")\nvprint(f\"  beam_size: {beam_size}\")\nvprint(f\"  sequence_type: {sequence_type}\")\nvprint(f\"  branch_model: {branch_model}\")\nvprint(f\"  window_coverage: {window_coverage}\")\n\nflag = 0\nsupport_format = ['.fas', '.phy', '.fasta', 'phylip']\nbio_format = ['fasta', 'phylip', 'fasta', 'phylip']\n\ntaxa_name = {}\n\nfor i in range(0, len(support_format)):\n    ele = support_format[i]\n    if msa_dir.endswith(ele):\n        flag = 1\n        try:\n            vprint(f\"[Fusang] Reading alignment file: {msa_dir} as {bio_format[i]}\")\n            alignment = AlignIO.read(open(msa_dir), bio_format[i])\n\n            len_of_msa = len(alignment[0].seq)\n            taxa_num = len(alignment)\n            vprint(f\"[Fusang] Alignment loaded. Number of taxa: {taxa_num}, MSA length: {len_of_msa}\")\n\n            save_alignment = save_prefix + '_fusang.fas'\n            with open(save_alignment,'w') as f:\n                for record in alignment:\n                    taxa_name[len(taxa_name)] = record.id\n                    f.write('>'+str(len(taxa_name)-1)+'\\n')\n                    f.write(str(record.seq)+'\\n')\n            vprint(f\"[Fusang] Alignment saved to: {save_alignment}\")\n        except Exception as e:\n            print('Something wrong about your msa file, please check your msa file')\n            if verbose:\n                print(f\"[Fusang] Exception: {e}\")\n        break\n\nif flag == 0:\n    print('we do not support this format of msa')\n    sys.exit(1)\n\nstart_end_list = [None, None, None]\n\nend = -1\nfor i in range(3, 100):\n    start = end + 1\n    end = start + int(comb_math(i,3)) - 1\n    start_end_list.append((start,end))\n\nid_for_taxa = [i for i in range(0, taxa_num)]\ncomb_of_id = list(combinations(id_for_taxa, 4))\ncomb_of_id.sort(key=lambda ele: ele[-1])\n\nleave_node_id = [i for i in range(0, taxa_num)]\nleave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\nleave_node_comb_id = comb_of_id\n\nleave_node_comb_name = []\n\ndic_for_leave_node_comb_name = {}\n\nfor ele in leave_node_comb_id:\n    term = [chr(ord(u'\\u4e00')+id) for id in ele]\n    dic_for_leave_node_comb_name[\"\".join(term)] = len(dic_for_leave_node_comb_name)\n    leave_node_comb_name.append(\"\".join(term))\n\ninternal_node_name_pool = ['internal_node_' + str(i) for i in range(3, 3000)]\n\nfusang_msa_dir = save_prefix + '_fusang.fas'\nvprint(f\"[Fusang] Converting alignment to numpy array...\")\norg_seq = get_numpy(fusang_msa_dir)\nos.remove(fusang_msa_dir)\nvprint(f\"[Fusang] Alignment numpy array created and temp file removed.\")\n\nwindow_number = 1\n\nmodel_dir = \"/kaggle/input/fusang-models/keras/default/1\"\nif len_of_msa <= 1210:\n    vprint(f\"[Fusang] Using DL model 240 for MSA length {len_of_msa}\")\n    dl_model = get_dl_model_240()\n    if sequence_type == 'standard' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_240/S1G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/S1G/best_weights_clas').expect_partial()\n    if sequence_type == 'standard' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: /len_240/S1U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/S1U/best_weights_clas').expect_partial()\n    if sequence_type == 'coding' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_240/C1G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/C1G/best_weights_clas').expect_partial()\n    if sequence_type == 'coding' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: /len_240/C1U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/C1U/best_weights_clas').expect_partial()\n    if sequence_type == 'noncoding' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_240/N1G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/N1G/best_weights_clas').expect_partial()\n    if sequence_type == 'noncoding' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: /len_240/N1U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_240/N1U/best_weights_clas').expect_partial()\n\n    window_number = int(len_of_msa * float(window_coverage) // 240 + 1) \n    vprint(f\"[Fusang] Window number set to {window_number}\")\n\nelif len_of_msa > 1210:\n    vprint(f\"[Fusang] Using DL model 1200 for MSA length {len_of_msa}\")\n    dl_model = get_dl_model_1200()\n    if sequence_type == 'standard' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_1200/S2G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/S2G/best_weights_clas').expect_partial()\n    if sequence_type == 'standard' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: /len_1200/S2U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/S2U/best_weights_clas').expect_partial()\n    if sequence_type == 'coding' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_1200/C2G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/C2G/best_weights_clas').expect_partial()\n    if sequence_type == 'coding' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: ./dl_model/len_1200/C2U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/C2U/best_weights_clas').expect_partial()\n    if sequence_type == 'noncoding' and branch_model == 'gamma':\n        vprint(\"[Fusang] Loading weights: /len_1200/N2G/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/N2G/best_weights_clas').expect_partial()\n    if sequence_type == 'noncoding' and branch_model == 'uniform':\n        vprint(\"[Fusang] Loading weights: /len_1200/N2U/best_weights_clas\")\n        dl_model.load_weights(filepath=model_dir+'/len_1200/N2U/best_weights_clas').expect_partial()\n\n    window_number = int(len_of_msa * float(window_coverage) // 1200 + 1) \n    vprint(f\"[Fusang] Window number set to {window_number}\")\n\n\nvprint(f\"[Fusang] Filling DL predictions...\")\n\ndl_predict = np.zeros((len(comb_of_id), 3))\nif len_of_msa > 1210:\n    fill_dl_predict(window_number)\n    dl_predict /= window_number\nelif len_of_msa <= 1210:\n    dl_predict = np.zeros((len(comb_of_id), 3))\n    fill_dl_predict_2(window_number)\n    dl_predict /= window_number\nelse:\n    print('current version of fusang do not support this length of MSA')\n    sys.exit(1)\n\nvprint(f\"[Fusang] DL predictions normalized by window number.\")\n\nif not os.path.exists('./dl_output/'):\n    vprint(f\"[Fusang] Creating output directory ./dl_output/\")\n    os.mkdir('./dl_output/')\n\nvprint(f\"[Fusang] Searching for best phylogenetic tree...\")\n\nif taxa_num > 10:\n    searched_tree = transform_str(gen_phylogenetic_tree_2(dl_predict, int(beam_size)), taxa_name)\nelse:\n    searched_tree = transform_str(gen_phylogenetic_tree(dl_predict, int(beam_size)), taxa_name)\nvprint(f\"[Fusang] Tree search complete.\")\n\nbuild_log = open('./dl_output/{}.txt'.format(save_prefix), 'a')\nbuild_log.write(searched_tree)\nbuild_log.close()\nvprint(f\"[Fusang] Tree written to ./dl_output/{save_prefix}.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T05:56:33.991284Z","iopub.status.idle":"2025-08-27T05:56:33.991634Z","shell.execute_reply.started":"2025-08-27T05:56:33.991436Z","shell.execute_reply":"2025-08-27T05:56:33.991452Z"}},"outputs":[],"execution_count":null}]}