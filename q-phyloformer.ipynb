{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12750561,"sourceType":"datasetVersion","datasetId":8060187},{"sourceId":12809530,"sourceType":"datasetVersion","datasetId":8090481}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install biopython\n!pip install ete3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:33:05.083621Z","iopub.execute_input":"2025-08-20T17:33:05.084317Z","iopub.status.idle":"2025-08-20T17:33:16.861000Z","shell.execute_reply.started":"2025-08-20T17:33:05.084289Z","shell.execute_reply":"2025-08-20T17:33:16.860320Z"}},"outputs":[{"name":"stdout","text":"Collecting biopython\n  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->biopython) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->biopython) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->biopython) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->biopython) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->biopython) (2024.2.0)\nDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: biopython\nSuccessfully installed biopython-1.85\nCollecting ete3\n  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: ete3\n  Building wheel for ete3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273786 sha256=b7b7acc371f974d3d1256b98f054cf2058ab31532ce9ee2d6da53eb3524a3b03\n  Stored in directory: /root/.cache/pip/wheels/dd/a8/60/0a29caa9f8ceb7316704be63c1578ab13c36668abb646366ac\nSuccessfully built ete3\nInstalling collected packages: ete3\nSuccessfully installed ete3-3.1.3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport numpy as np\nimport functools\nimport argparse\nfrom Bio import AlignIO\nfrom itertools import combinations\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" \nimport math\nimport random\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, Sequential, regularizers\nimport torch\nfrom ete3 import Tree\nfrom pandas.core.frame import DataFrame\n\nglobal org_seq, comb_of_id, qp_model, qp_predict, len_of_msa, dic_for_leave_node_comb_name, start_end_list\nglobal taxa_num, leave_node_id, leave_node_name, leave_node_comb_id, leave_node_comb_name, internal_node_name_pool\n\nfrom multiprocessing import Process, Pool\nimport multiprocessing\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\nimport random\nimport os\nfrom Bio import SeqIO, Phylo\nimport csv\nfrom itertools import combinations\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:35:08.419801Z","iopub.execute_input":"2025-08-20T17:35:08.420097Z","iopub.status.idle":"2025-08-20T17:35:08.453011Z","shell.execute_reply.started":"2025-08-20T17:35:08.420070Z","shell.execute_reply":"2025-08-20T17:35:08.452496Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### [Fusang] Util functions\n- comb_math\n- nlargest_indices\n- get_quartet_ID","metadata":{}},{"cell_type":"code","source":"def comb_math(n,m):\n    return math.factorial(n)//(math.factorial(n-m)*math.factorial(m))\n\n\ndef nlargest_indices(arr, n):\n    uniques = np.unique(arr)\n    threshold = uniques[-n]\n    return np.where(arr >= threshold)\n\n\ndef get_quartet_ID(quartet):\n    return \"\".join(sorted(quartet))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.600757Z","iopub.status.idle":"2025-08-20T17:32:42.600978Z","shell.execute_reply.started":"2025-08-20T17:32:42.600877Z","shell.execute_reply":"2025-08-20T17:32:42.600888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Tree from quartet","metadata":{}},{"cell_type":"code","source":"def tree_from_quartet(quartet):\n    root = Tree()\n    root.name = \"internal_node_0\"\n    left = root.add_child(name=\"internal_node_1\")\n    left.add_child(name=quartet[0])\n    left.add_child(name=quartet[1])\n    right = root.add_child(name=\"internal_node_2\")\n    right.add_child(name=quartet[2])\n    right.add_child(name=quartet[3])\n    for desc in root.iter_descendants():\n        desc.dist = 0\n    return root","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.601862Z","iopub.status.idle":"2025-08-20T17:32:42.602098Z","shell.execute_reply.started":"2025-08-20T17:32:42.601967Z","shell.execute_reply":"2025-08-20T17:32:42.601976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Getting topology ID\n- Given a quartet\n- Given a quartet key and 2 clusters","metadata":{}},{"cell_type":"code","source":"def get_topology_ID(quartet):\n    return get_quartet_ID(quartet[0:2]) + get_quartet_ID(quartet[2:4])\n\n\ndef get_current_topology_id(quart_key, cluster_1, cluster_2):\n    ans = []\n    a1 = quart_key.index(cluster_1)\n    a2 = quart_key.index(cluster_2)\n    ans.append(str(a1))\n    ans.append(str(a2))\n    ans = set(ans)\n    if ans == {'0', '1'} or ans == {'2', '3'}:\n        return 0\n    elif ans == {'0', '2'} or ans == {'1', '3'}:\n        return 1\n    elif ans == {'0', '3'} or ans == {'1', '2'}:\n        return 2\n    else:\n        print('Error of function get_current_topology_id, exit the program')\n        sys.exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.603646Z","iopub.status.idle":"2025-08-20T17:32:42.603916Z","shell.execute_reply.started":"2025-08-20T17:32:42.603782Z","shell.execute_reply":"2025-08-20T17:32:42.603800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Tree Functions\n- judge_tree_score\n- get_modify_tree","metadata":{}},{"cell_type":"code","source":"def judge_tree_score(tree, quart_distribution, new_addition_taxa, dic_for_leave_node_comb_name):\n    '''\n        parameter\n        tree: a candidate tree, can be any taxas\n        quart_distribution: the prob distribution of the topology of every 4-taxa\n    '''\n    crt_tree = tree.copy(\"newick\")\n    leaves = crt_tree.get_leaves()\n\n    leaves = [ele.name for ele in leaves]\n    total_quarts = list(combinations(leaves, 4))\n    quarts = []\n    for ele in total_quarts:\n        if new_addition_taxa in ele:\n            quarts.append(ele)\n\n    total_quart_score = 0\n\n    for quart in quarts:\n        crt_tree = tree.copy(\"newick\")\n        try:\n            crt_tree.prune(list(quart))\n        except:\n            print('Error of pruning 4 taxa from current tree, the current tree is:')\n            print(crt_tree)\n            sys.exit(0)\n\n        quart_key = \"\".join(sorted(list(quart)))\n        #quart_topo_id = leave_node_comb_name.index(quart_key)\n\n        quart_topo_id = dic_for_leave_node_comb_name[quart_key]\n\n        quart_topo_distribution = quart_distribution[quart_topo_id]\n\n        # judge current tree belongs to which topology\n        tmp = re.findall(\"\\([\\s\\S]\\,[\\s\\S]\\)\", crt_tree.write(format=9))[0]\n        topology_id = get_current_topology_id(quart_key, tmp[1], tmp[3])\n\n        total_quart_score += np.log(quart_topo_distribution[topology_id]+1e-200)\n\n    return total_quart_score\n\n\ndef get_modify_tree(tmp_tree, edge_0, edge_1, new_add_node_name):\n    '''\n        add a new leave node between edge_0 and edge_1\n        default: edge_0 is the parent node of edge_1\n    '''\n    modify_tree = tmp_tree.copy(\"newick\")\n    if edge_0 != edge_1:\n        new_node = Tree()\n        new_node.add_child(name=new_add_node_name)\n        detached_node = modify_tree&edge_1\n        detached = detached_node.detach()\n        inserted_node = modify_tree&edge_0\n        inserted_node.add_child(new_node)\n        new_node.add_child(detached_node)\n\n    else:\n        modify_tree.add_child(name=new_add_node_name)\n\n    return modify_tree\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.605072Z","iopub.status.idle":"2025-08-20T17:32:42.605315Z","shell.execute_reply.started":"2025-08-20T17:32:42.605202Z","shell.execute_reply":"2025-08-20T17:32:42.605215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Search branch function","metadata":{}},{"cell_type":"code","source":"def search_this_branch(tmp_tree, edge_0, edge_1, current_quartets, current_leave_node_name, queue, dic_for_leave_node_comb_name):\n    modify_tree = get_modify_tree(tmp_tree, edge_0, edge_1, current_leave_node_name)\n    modify_tree.resolve_polytomy(recursive=True)\n    modify_tree.unroot()\n    tmp_tree_score = judge_tree_score(modify_tree, current_quartets, current_leave_node_name, dic_for_leave_node_comb_name)\n\n    dic = {}\n    dic['tree'] = modify_tree\n    dic['score'] = tmp_tree_score\n    queue.put(dic)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.606585Z","iopub.status.idle":"2025-08-20T17:32:42.606886Z","shell.execute_reply.started":"2025-08-20T17:32:42.606726Z","shell.execute_reply":"2025-08-20T17:32:42.606740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Mask Functions","metadata":{}},{"cell_type":"code","source":"def select_mask_node_pair(qp_predict, new_add_taxa):\n    \n    if new_add_taxa <= 9:\n        return None\n\n    mask_node_pair = []\n    \n    current_start = start_end_list[new_add_taxa][0]\n    current_end = start_end_list[new_add_taxa][1]\n    select_distribution = qp_predict[current_start:current_end+1]\n    if np.max(select_distribution) < 0.90:\n        return None\n    else:\n        x,y = nlargest_indices(select_distribution, int(max(10,0.01*len(select_distribution)))) \n        \n    for i in range(0,len(x)):\n        idx = x[i]\n        topology_value = y[i]\n        quartet_comb = comb_of_id[current_start+idx]\n        \n        if topology_value == 0:\n            mask_node_pair.append((quartet_comb[0],quartet_comb[1]))\n        if topology_value == 1:\n            mask_node_pair.append((quartet_comb[0],quartet_comb[2]))\n        if topology_value == 2:\n            mask_node_pair.append((quartet_comb[1],quartet_comb[2]))\n            \n    return mask_node_pair\n\n\ndef mask_edge(tree,node1,node2,edge_list):\n    # mask edge between node1 and node2\n\n    if len(edge_list) <= 3:\n        return edge_list\n\n    ancestor_name = tree.get_common_ancestor(node1,node2).name\n    remove_edge = []\n\n    node = tree.search_nodes(name=node1)[0]\n    while node:\n        if node.name == ancestor_name:\n            break\n\n        edge_0 = node.up.name\n        edge_1 = node.name\n\n        if len(remove_edge) >= len(edge_list) - 3:\n            break\n        remove_edge.append((edge_0, edge_1))\n        node = node.up\n\n    node = tree.search_nodes(name=node2)[0]\n    while node:\n        if node.name == ancestor_name:\n            break\n\n        edge_0 = node.up.name\n        edge_1 = node.name\n\n        if len(remove_edge) >= len(edge_list) - 3:\n            break\n        remove_edge.append((edge_0, edge_1))\n        node = node.up\n\n    for ele in remove_edge:\n        if ele in edge_list:\n            edge_list.remove(ele)\n\n    return edge_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.608165Z","iopub.status.idle":"2025-08-20T17:32:42.608470Z","shell.execute_reply.started":"2025-08-20T17:32:42.608316Z","shell.execute_reply":"2025-08-20T17:32:42.608332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Gen_phylogen_tree\nsearch phylo tree having highest score","metadata":{}},{"cell_type":"code","source":"def gen_phylogenetic_tree(current_quartets, beam_size):\n    '''\n        search the phylogenetic tree having highest score\n        idx: the name of numpy file \n    '''\n    current_leave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n    \n    candidate_tree_beam = []\n\n    quartet_id = leave_node_comb_name[0]\n\n    for _label in [0, 1, 2]:\n        if _label == 0:\n            label_id = \"\".join([quartet_id[0], quartet_id[1], quartet_id[2], quartet_id[3]])\n        elif _label == 1:\n            label_id = \"\".join([quartet_id[0], quartet_id[2], quartet_id[1], quartet_id[3]])\n        elif _label == 2:\n            label_id = \"\".join([quartet_id[0], quartet_id[3], quartet_id[1], quartet_id[2]])\n        \n        _tree = tree_from_quartet(label_id)\n        _tree.unroot()\n\n        _tree_score = current_quartets[0, _label]\n\n        tmp_tree_dict = {'Tree':_tree, 'tree_score':_tree_score}\n        candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n\n    idx_for_internal_node_name_pool = 0\n\n    current_tree_score_beam = []\n    optim_tree_beam = []\n\n    #in the start point set beam size equal to 3\n    for i in range(0, 3):\n        current_tree_score_beam.append(candidate_tree_beam[i]['tree_score'])\n        optim_tree_beam.append(candidate_tree_beam[i]['Tree'])\n    \n    for i in range(4, len(current_leave_node_name)):\n        candidate_tree_beam = []\n\n        for j in range(0, len(optim_tree_beam)):\n            ele = optim_tree_beam[j]\n\n            idx_for_this_iter = 0\n\n            edge_0_list = []\n            edge_1_list = []\n\n            if ele == None:\n                continue\n            optim_tree = ele.copy(\"newick\")\n\n            for node in optim_tree.iter_descendants():\n                tmp_tree = optim_tree.copy(\"newick\")\n                edge_0 = node.up.name\n                edge_1 = node.name\n                if edge_0 == '' or edge_1 == '':\n                    continue\n\n                else:\n                    edge_0_list.append(edge_0)\n                    edge_1_list.append(edge_1)\n\n            queue = multiprocessing.Manager().Queue()\n            para_list = [(tmp_tree, edge_0_list[k], edge_1_list[k], current_quartets, current_leave_node_name[i], queue, dic_for_leave_node_comb_name) for k in range(0, len(edge_0_list))]\n\n            process_num = min(64, 2*i+6)\n            pool = Pool(process_num)\n            pool.starmap(search_this_branch, para_list)\n            pool.close()\n            pool.join()\n\n            candidate_tree_score = []\n            candidate_tree = []\n\n            while not queue.empty():\n                tmp_dic = queue.get()\n                \n                tmp_tree_dict = {'Tree':tmp_dic['tree'], 'tree_score':tmp_dic['score']+current_tree_score_beam[j]}\n                candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n        candidate_tree_beam = candidate_tree_beam[0:beam_size]\n        \n        optim_tree_beam = []\n        current_tree_score_beam = []\n        for ele in candidate_tree_beam:\n            crt_tree = ele['Tree'].copy(\"newick\") \n            for node in crt_tree.traverse(\"preorder\"):\n                if node.name == '':\n                    node.name = str(internal_node_name_pool[idx_for_internal_node_name_pool])\n                    idx_for_internal_node_name_pool += 1\n            optim_tree_beam.append(crt_tree)\n            crt_tree_score = ele['tree_score']\n            current_tree_score_beam.append(crt_tree_score)\n            \n    return optim_tree_beam[0].write(format=9)\n\n\ndef gen_phylogenetic_tree_2(current_quartets, beam_size):\n    '''\n        search the phylogenetic tree having highest score\n        idx: the name of numpy file \n    '''\n    current_leave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n    \n    candidate_tree_beam = []\n\n    quartet_id = leave_node_comb_name[0]\n\n    for _label in [0, 1, 2]:\n        if _label == 0:\n            label_id = \"\".join([quartet_id[0], quartet_id[1], quartet_id[2], quartet_id[3]])\n        elif _label == 1:\n            label_id = \"\".join([quartet_id[0], quartet_id[2], quartet_id[1], quartet_id[3]])\n        elif _label == 2:\n            label_id = \"\".join([quartet_id[0], quartet_id[3], quartet_id[1], quartet_id[2]])\n        \n        _tree = tree_from_quartet(label_id)\n        _tree.unroot()\n\n        _tree_score = current_quartets[0, _label]\n\n        tmp_tree_dict = {'Tree':_tree, 'tree_score':_tree_score}\n        candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n\n    idx_for_internal_node_name_pool = 0\n\n    current_tree_score_beam = []\n    optim_tree_beam = []\n\n    #in the start point set beam size equal to 3\n    for i in range(0, 3):\n        current_tree_score_beam.append(candidate_tree_beam[i]['tree_score'])\n        optim_tree_beam.append(candidate_tree_beam[i]['Tree'])\n    \n    for i in range(4, len(current_leave_node_name)):\n        candidate_tree_beam = []\n\n        for j in range(0, len(optim_tree_beam)):\n            ele = optim_tree_beam[j]\n\n            idx_for_this_iter = 0\n\n            edge_0_list = []\n            edge_1_list = []\n\n            if ele == None:\n                continue\n            optim_tree = ele.copy(\"newick\")\n\n            for node in optim_tree.iter_descendants():\n                tmp_tree = optim_tree.copy(\"newick\")\n                edge_0 = node.up.name\n                edge_1 = node.name\n                if edge_0 == '' or edge_1 == '':\n                    continue\n\n                else:\n                    edge_0_list.append(edge_0)\n                    edge_1_list.append(edge_1)\n\n            edge_list = [(edge_0_list[i], edge_1_list[i]) for i in range(0, len(edge_0_list))]\n            mask_node_pairs = select_mask_node_pair(current_quartets, i)\n\n            if mask_node_pairs != None:\n\n                mask_node_pairs = list(set(mask_node_pairs))\n                for node_pairs in mask_node_pairs:\n                    node1 = chr(ord(u'\\u4e00')+node_pairs[0])\n                    node2 = chr(ord(u'\\u4e00')+node_pairs[1])\n                    \n                    edge_list = mask_edge(ele.copy(\"deepcopy\"),node1,node2,edge_list)\n                    if len(edge_list) <= 3:\n                        break\n\n            edge_0_list = [ele[0] for ele in edge_list]\n            edge_1_list = [ele[1] for ele in edge_list]\n\n            queue = multiprocessing.Manager().Queue()\n            para_list = [(tmp_tree, edge_0_list[k], edge_1_list[k], current_quartets, current_leave_node_name[i], queue, dic_for_leave_node_comb_name) for k in range(0, len(edge_0_list))]\n\n            process_num = min(64, len(edge_0_list))\n            pool = Pool(process_num)\n            pool.starmap(search_this_branch, para_list)\n            pool.close()\n            pool.join()\n\n            candidate_tree_score = []\n            candidate_tree = []\n\n            while not queue.empty():\n                tmp_dic = queue.get()\n                \n                tmp_tree_dict = {'Tree':tmp_dic['tree'], 'tree_score':tmp_dic['score']+current_tree_score_beam[j]}\n                candidate_tree_beam.append(tmp_tree_dict)\n\n        candidate_tree_beam.sort(key=lambda k: -k['tree_score'])\n        candidate_tree_beam = candidate_tree_beam[0:beam_size]\n        \n        optim_tree_beam = []\n        current_tree_score_beam = []\n        for ele in candidate_tree_beam:\n            crt_tree = ele['Tree'].copy(\"newick\") \n            for node in crt_tree.traverse(\"preorder\"):\n                if node.name == '':\n                    node.name = str(internal_node_name_pool[idx_for_internal_node_name_pool])\n                    idx_for_internal_node_name_pool += 1\n            optim_tree_beam.append(crt_tree)\n            crt_tree_score = ele['tree_score']\n            current_tree_score_beam.append(crt_tree_score)\n\n    return optim_tree_beam[0].write(format=9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.645849Z","iopub.execute_input":"2025-08-20T17:32:42.646146Z","iopub.status.idle":"2025-08-20T17:32:42.674450Z","shell.execute_reply.started":"2025-08-20T17:32:42.646119Z","shell.execute_reply":"2025-08-20T17:32:42.673627Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### [Fusang] Util functions\n- transform_str\n- cmp\n- get_numpy","metadata":{}},{"cell_type":"code","source":"def transform_str(str_a, taxa_name):\n    str_b = ''\n    id_set = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\n\n    for i in range(0, len(str_a)):\n        if str_a[i] in id_set:\n            str_b += taxa_name[ord(str_a[i])-ord(u'\\u4e00')]\n        else:\n            str_b += str_a[i]\n\n    return str_b\n\n\ndef cmp(a, b):\n    if int(b) > int(a):\n        return -1\n    if int(a) < int(b):\n        return 1\n    return 0\n\n\ndef get_numpy(aln_file):\n    '''\n        current version only supports the total length of msa less than 10K\n    '''\n    aln = open(aln_file)\n    dic = {'A':'0','T':'1','C':'2','G':'3','-':'4', 'N':'4'}\n\n    # for masking other unknown bases\n    other_base = ['R', 'Y', 'K', 'M', 'U', 'S', 'W', 'B', 'D', 'H', 'V', 'X']\n    for ele in other_base:\n        dic[ele] = '4'\n\n    matrix_out=[]\n    fasta_dic={}\n    for line in aln:\n        if line[0]==\">\":\n            header=line[1:].rstrip('\\n').strip()\n            fasta_dic[header]=[]\n        elif line[0].isalpha() or line[0]=='-':\n            for base, num in dic.items():\n                line=line[:].rstrip('\\n').strip().replace(base,num)\n            line=list(line)\n            line=[int(n) for n in line]\n            fasta_dic[header] += line+[4]*(14000-len(line))\n\n    taxa_block=[]\n    for taxa in sorted(list(fasta_dic.keys()), key=functools.cmp_to_key(cmp)):\n        taxa_block.append(fasta_dic[taxa.strip()])\n    fasta_dic={}\n    matrix_out.append(taxa_block)\n\n    return np.array(matrix_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.675569Z","iopub.execute_input":"2025-08-20T17:32:42.675807Z","iopub.status.idle":"2025-08-20T17:32:42.693251Z","shell.execute_reply.started":"2025-08-20T17:32:42.675783Z","shell.execute_reply":"2025-08-20T17:32:42.692629Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### [QPhyloformer] Defining QP models for different lenghts (>1200 or >240)","metadata":{}},{"cell_type":"code","source":"class RowAttention(nn.Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n\n    def forward(self, x):\n        # x: (B, N=4, L, D)\n        B, N, L, D = x.shape\n        x = x.transpose(1, 2)         # (B, L, N, D)\n        x = x.reshape(B * L, N, D)\n        x, _ = self.attn(x, x, x)\n        x = x.reshape(B, L, N, D).transpose(1, 2)  # (B, N, L, D)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:34:24.921587Z","iopub.execute_input":"2025-08-20T17:34:24.922467Z","iopub.status.idle":"2025-08-20T17:34:24.926942Z","shell.execute_reply.started":"2025-08-20T17:34:24.922440Z","shell.execute_reply":"2025-08-20T17:34:24.926299Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ColumnAttention(nn.Module):\n    def __init__(self, dim, heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n\n    def forward(self, x):\n        # x: (B, N=4, L, D)\n        B, N, L, D = x.shape\n        x = x.reshape(B * N, L, D)\n        x, _ = self.attn(x, x, x)\n        x = x.reshape(B, N, L, D)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:34:28.457697Z","iopub.execute_input":"2025-08-20T17:34:28.457980Z","iopub.status.idle":"2025-08-20T17:34:28.462656Z","shell.execute_reply.started":"2025-08-20T17:34:28.457958Z","shell.execute_reply":"2025-08-20T17:34:28.461850Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class MSABlock(nn.Module):\n    def __init__(self, dim, heads, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n\n        self.row_attn = RowAttention(dim, heads)\n        self.col_attn = ColumnAttention(dim, heads)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim), # \n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        # Row Attention + residual\n        x = x + self.row_attn(self.norm1(x))\n        # Column Attention + residual\n        x = x + self.col_attn(self.norm2(x))\n        # Feed‑Forward + residual\n        x = x + self.ffn(self.norm3(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:35:48.580662Z","iopub.execute_input":"2025-08-20T17:35:48.581211Z","iopub.status.idle":"2025-08-20T17:35:48.586807Z","shell.execute_reply.started":"2025-08-20T17:35:48.581187Z","shell.execute_reply":"2025-08-20T17:35:48.586097Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class QPhyloformer(nn.Module):\n    def __init__(self, dim=64, heads=4, layers_num=4):\n        super().__init__()\n        self.embedding = nn.Embedding(VOCAB_SIZE, dim)\n        self.taxon_embed = nn.Embedding(4, dim)  # 4 fixed taxa\n        self.blocks = nn.ModuleList([\n            MSABlock(dim, heads) for _ in range(layers_num)\n        ])\n        self.norm = nn.LayerNorm(dim)\n\n        # (B, D, 4, L)\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),  # pool over taxa × sites -> (B, D, 1, 1)\n            nn.Flatten(), # (B, D)\n            nn.Linear(dim, 3),  # (B, 3)\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        # x: (B, 4, L), each entry ∈ [0, VOCAB_SIZE-1]\n        if isinstance(x, np.ndarray):\n            x = torch.from_numpy(x)\n        x = x.long()\n        # print(f\"Input shape: {x.shape}\")\n        B, N, L = x.shape\n        x = self.embedding(x)  # (B, 4, L, D)\n        taxon_ids = torch.arange(N, device=x.device).unsqueeze(0).expand(B, N)\n        taxon_embed = self.taxon_embed(taxon_ids).unsqueeze(2)  # (B, 4, 1, D)\n        x = x + taxon_embed  # (B, 4, L, D)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.norm(x)\n\n        x = x.permute(0, 3, 1, 2)  # (B, D, 4, L) -> Quick Fix. Need to recheck.\n        prediction = self.classifier(x)  # (B, 3)\n        return prediction\n    \n    def load_weights(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n        self.to(torch.device('cpu'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:35:51.618802Z","iopub.execute_input":"2025-08-20T17:35:51.619133Z","iopub.status.idle":"2025-08-20T17:35:51.626694Z","shell.execute_reply.started":"2025-08-20T17:35:51.619107Z","shell.execute_reply":"2025-08-20T17:35:51.625983Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_qp_model(seq_len=512, dim=64, heads=4, layers_num=4):\n    return QPhyloformer(seq_len=seq_len, dim=dim, heads=heads, layers_num=layers_num)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:35:55.377920Z","iopub.execute_input":"2025-08-20T17:35:55.378707Z","iopub.status.idle":"2025-08-20T17:35:55.382167Z","shell.execute_reply.started":"2025-08-20T17:35:55.378680Z","shell.execute_reply":"2025-08-20T17:35:55.381459Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### [Fusang] Predict Functions (slide_window)","metadata":{}},{"cell_type":"code","source":"def fill_qp_predict_each_slide_window(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 1200))\n\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n        test_seq = torch.from_numpy(batch_seq).long()\n        predicted = qp_model(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            qp_predict[i*50000+j,:] += predicted[j,:]\n        #qp_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 1200))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = torch.from_numpy(batch_seq).long()\n    predicted = qp_model(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        qp_predict[iters*50000+j,:] += predicted[j,:]\n    #qp_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]\n    \ndef fill_qp_predict_each_slide_window_2(len_idx_1, len_idx_2):\n    start_pos = 0\n    iters = len(comb_of_id) // 50000\n    for i in range(0, iters):\n        batch_seq = np.zeros((50000, 4, 240))\n        for j in range(0, len(batch_seq)):\n            idx = np.array(comb_of_id[i*50000+j])\n            batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n        \n        # test_seq = tf.expand_dims(batch_seq.astype(np.float32), axis=-1)\n        test_seq = torch.from_numpy(batch_seq).long()  # shape: (50000, 4, 240)\n        predicted = qp_model(x=test_seq)\n\n        for j in range(0, len(batch_seq)):\n            qp_predict[i*50000+j,:] += predicted[j,:]\n        #qp_predict[i*50000:i*50000+len(batch_seq),:] += predicted[:,:]\n\n        start_pos += 50000\n\n    last_batch_size = len(comb_of_id) % 50000\n    batch_seq = np.zeros((last_batch_size, 4, 240))\n\n    for j in range(0, len(batch_seq)):\n        idx = np.array(comb_of_id[iters*50000+j])\n        batch_seq[j] = org_seq[0, idx[:], len_idx_1:len_idx_2]\n\n    test_seq = torch.from_numpy(batch_seq).long()\n    predicted = qp_model(x=test_seq)\n\n    for j in range(0, len(batch_seq)):\n        qp_predict[iters*50000+j,:] += predicted[j,:]\n    #qp_predict[iters*50000:iters*50000+len(batch_seq),:] += predicted[:,:]\n\ndef fill_qp_predict(window_number):\n    step = (len_of_msa - 1200) // window_number\n    start_idx = 0\n    for i in range(0, window_number):\n        end_idx = start_idx + 1200\n        fill_qp_predict_each_slide_window(start_idx, end_idx)\n        start_idx += step\n        \ndef fill_qp_predict_2(window_number):\n    if len_of_msa > 240:\n        step = (len_of_msa - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_qp_predict_each_slide_window_2(start_idx, end_idx)\n            start_idx += step\n\n    else:\n        step = (250 - 240) // window_number\n        start_idx = 0\n        for i in range(0, window_number):\n            end_idx = start_idx + 240\n            fill_qp_predict_each_slide_window_2(start_idx, end_idx)\n            start_idx += step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:32:42.722611Z","iopub.status.idle":"2025-08-20T17:32:42.722913Z","shell.execute_reply.started":"2025-08-20T17:32:42.722764Z","shell.execute_reply":"2025-08-20T17:32:42.722778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [Fusang] Main Function","metadata":{}},{"cell_type":"code","source":"# if __name__ == '__main__':\n# parser = argparse.ArgumentParser('get_msa_dir')\n# p_input = parser.add_argument_group(\"INPUT\")\n# p_input.add_argument(\"--msa_dir\", action=\"store\", type=str, required=True)\n# p_input.add_argument(\"--save_prefix\", action=\"store\", type=str, required=True)\n# p_input.add_argument(\"--beam_size\", action=\"store\", type=str, default='1', required=False)\n# p_input.add_argument(\"--sequence_type\", action=\"store\", type=str, default='standard', required=False)\n# p_input.add_argument(\"--branch_model\", action=\"store\", type=str, default='gamma', required=False)\n# p_input.add_argument(\"--window_coverage\", action=\"store\", type=str, default='1', required=False)\n# p_input.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output.\")\n# args = parser.parse_args()\n# msa_dir = args.msa_dir\n# save_prefix = args.save_prefix\n# beam_size = args.beam_size\n# sequence_type = args.sequence_type\n# branch_model = args.branch_model\n# window_coverage = args.window_coverage\n# verbose = args.verbose\n\nmsa_dir = \"/kaggle/input/sample-msa/a.fas\"\nsave_prefix = \"tree_for_a\"\nbeam_size = '1'\nsequence_type = 'standard'\nbranch_model = 'gamma'\nwindow_coverage = '1'\nverbose = True\n\n\ndef vprint(*a, **kw):\n    if verbose:\n        print(*a, **kw)\n\nvprint(\"[QPhyloformer] Starting workflow with arguments:\")\nvprint(f\"  msa_dir: {msa_dir}\")\nvprint(f\"  save_prefix: {save_prefix}\")\nvprint(f\"  beam_size: {beam_size}\")\nvprint(f\"  sequence_type: {sequence_type}\")\nvprint(f\"  branch_model: {branch_model}\")\nvprint(f\"  window_coverage: {window_coverage}\")\n\nflag = 0\nsupport_format = ['.fas', '.phy', '.fasta', 'phylip']\nbio_format = ['fasta', 'phylip', 'fasta', 'phylip']\n\ntaxa_name = {}\n\nfor i in range(0, len(support_format)):\n    ele = support_format[i]\n    if msa_dir.endswith(ele):\n        flag = 1\n        try:\n            vprint(f\"[QPhyloformer] Reading alignment file: {msa_dir} as {bio_format[i]}\")\n            alignment = AlignIO.read(open(msa_dir), bio_format[i])\n\n            len_of_msa = len(alignment[0].seq)\n            taxa_num = len(alignment)\n            vprint(f\"[QPhyloformer] Alignment loaded. Number of taxa: {taxa_num}, MSA length: {len_of_msa}\")\n\n            save_alignment = save_prefix + '_fusang.fas'\n            with open(save_alignment,'w') as f:\n                for record in alignment:\n                    taxa_name[len(taxa_name)] = record.id\n                    f.write('>'+str(len(taxa_name)-1)+'\\n')\n                    f.write(str(record.seq)+'\\n')\n            vprint(f\"[QPhyloformer] Alignment saved to: {save_alignment}\")\n        except Exception as e:\n            print('Something wrong about your msa file, please check your msa file')\n            if verbose:\n                print(f\"[QPhyloformer] Exception: {e}\")\n        break\n\nif flag == 0:\n    print('we do not support this format of msa')\n    sys.exit(1)\n\nstart_end_list = [None, None, None]\n\nend = -1\nfor i in range(3, 100):\n    start = end + 1\n    end = start + int(comb_math(i,3)) - 1\n    start_end_list.append((start,end))\n\nid_for_taxa = [i for i in range(0, taxa_num)]\ncomb_of_id = list(combinations(id_for_taxa, 4))\ncomb_of_id.sort(key=lambda ele: ele[-1])\n\nleave_node_id = [i for i in range(0, taxa_num)]\nleave_node_name = [chr(ord(u'\\u4e00')+i) for i in range(0, taxa_num)]\nleave_node_comb_id = comb_of_id\n\nleave_node_comb_name = []\n\ndic_for_leave_node_comb_name = {}\n\nfor ele in leave_node_comb_id:\n    term = [chr(ord(u'\\u4e00')+id) for id in ele]\n    dic_for_leave_node_comb_name[\"\".join(term)] = len(dic_for_leave_node_comb_name)\n    leave_node_comb_name.append(\"\".join(term))\n\ninternal_node_name_pool = ['internal_node_' + str(i) for i in range(3, 3000)]\n\nfusang_msa_dir = save_prefix + '_fusang.fas'\nvprint(f\"[QPhyloformer] Converting alignment to numpy array...\")\norg_seq = get_numpy(fusang_msa_dir)\nos.remove(fusang_msa_dir)\nvprint(f\"[QPhyloformer] Alignment numpy array created and temp file removed.\")\n\nwindow_number = 1\n\nif len_of_msa <= 1210:\n    qp_model = get_qp_model(seq_len=len_of_msa, dim=64, heads=4, layers_num=4)\n    vprint(\"[QPhyloformer] Loading weights: ./dl_model/qphyloformer/quartet_model_240.pt\")\n    qp_model.load_weights(filepath='./dl_model/qphyloformer/quartet_model_240.pt')\n    window_number = int(len_of_msa * float(window_coverage) // 240 + 1) \n    vprint(f\"[QPhyloformer] Window number set to {window_number}\")\nelif len_of_msa > 1210:\n    qp_model = get_qp_model(seq_len=len_of_msa, dim=64, heads=4, layers_num=4)\n    vprint(\"[QPhyloformer] Loading weights: ./dl_model/qphyloformer/quartet_model_1200.pt\")\n    qp_model.load_weights(filepath='./dl_model/qphyloformer/quartet_model_1200.pt')\n    window_number = int(len_of_msa * float(window_coverage) // 1200 + 1) \n    vprint(f\"[QPhyloformer] Window number set to {window_number}\")\n\nvprint(f\"[QPhyloformer] Filling QP predictions...\")\n\nqp_predict = torch.zeros((len(comb_of_id), 3), dtype=torch.float32)\n\nif len_of_msa <= 1210:\n    fill_qp_predict(window_number)\n    qp_predict /= window_number\nelif len_of_msa > 1210:\n    fill_qp_predict_2(window_number)\n    qp_predict /= window_number\nelse:\n    print('current version of qphyloformer do not support this length of MSA')\n    sys.exit(1)\n\nvprint(f\"[QPhyloformer] QP predictions normalized by window number.\")\n\nif not os.path.exists('./qp_output/'):\n    vprint(f\"[QPhyloformer] Creating output directory ./qp_output/\")\n    os.mkdir('./qp_output/')\n\nvprint(f\"[QPhyloformer] Searching for best phylogenetic tree...\")\n\n# Print the quartets in qp_predict\n\nqp_predict_np = qp_predict.detach().cpu().numpy()\nif taxa_num > 10:\n    searched_tree = transform_str(gen_phylogenetic_tree_2(qp_predict_np, int(beam_size)), taxa_name)\nelse:\n    searched_tree = transform_str(gen_phylogenetic_tree(qp_predict_np, int(beam_size)), taxa_name)\nvprint(f\"[QPhyloformer] Tree search complete.\")\n\nbuild_log = open('./qp_output/{}.txt'.format(save_prefix), 'a')\nbuild_log.write(searched_tree)\nbuild_log.close()\nvprint(f\"[QPhyloformer] Tree written to ./qp_output/{save_prefix}.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### [QPhyloformer] Training","metadata":{}},{"cell_type":"code","source":"def derive_quartets_from_full_msa(msa_folder, label_folder, msa_format='fasta'):\n    \"\"\"\n    Reads a folder of MSA files and a CSV file with labels, then derives quartets and their labels.\n    msa_folder: path to folder containing MSA files (.npy)\n    label_folder: path to folder containing label files (.npy)\n    Returns: list of (encoded_quartet, quartet_label)\n    \"\"\"\n    quartet_data = []\n\n    msa_files = sorted(os.listdir(msa_folder))\n    label_files = sorted(os.listdir(label_folder))\n\n    print(f\"[Data] Found {len(msa_files)} MSA files and {len(label_files)} label files\")\n\n    # wrap with tqdm\n    for seq_file, label_file in tqdm(zip(msa_files, label_files),\n                                     total=min(len(msa_files), len(label_files)),\n                                     desc=\"Loading Quartets\"):\n        label_path = os.path.join(label_folder, label_file)\n        label = np.load(label_path, allow_pickle=True)\n\n        seq_path = os.path.join(msa_folder, seq_file)\n        seq = np.load(seq_path, allow_pickle=True).squeeze(axis=0)\n\n        quartet_data.append((seq, label))\n\n    return quartet_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:36:02.293544Z","iopub.execute_input":"2025-08-20T17:36:02.294243Z","iopub.status.idle":"2025-08-20T17:36:02.299221Z","shell.execute_reply.started":"2025-08-20T17:36:02.294221Z","shell.execute_reply":"2025-08-20T17:36:02.298564Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss, correct = 0, 0\n\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (logits.argmax(dim=1) == y).sum().item()\n\n    return total_loss / len(dataloader.dataset), correct / len(dataloader.dataset)\n\n\nclass QuartetDataset(Dataset):\n    def __init__(self, quartet_data):\n        self.data = quartet_data\n    def __getitem__(self, idx):\n        return self.data[idx]\n    def __len__(self):\n        return len(self.data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:36:06.454433Z","iopub.execute_input":"2025-08-20T17:36:06.455218Z","iopub.status.idle":"2025-08-20T17:36:06.460439Z","shell.execute_reply.started":"2025-08-20T17:36:06.455191Z","shell.execute_reply":"2025-08-20T17:36:06.459835Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"BASE2IDX = {'A': 0, 'C': 1, 'G': 2, 'T': 3, '-': 4, 'N': 5}\nVOCAB_SIZE = len(BASE2IDX)\nfrom tqdm import tqdm\nfrom torch.utils.data import random_split\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = QPhyloformer().to(device)\n\nquartet_data = derive_quartets_from_full_msa(\n    '/kaggle/input/fusang/S1G/numpy_file/seq', \n    '/kaggle/input/fusang/S1G/numpy_file/label'\n)\n\ntotal_size = len(quartet_data)\ntrain_size = int(0.7 * total_size)\nval_size   = int(0.15 * total_size)\ntest_size  = total_size - train_size - val_size\n\ntrain_data, val_data, test_data = random_split(\n    quartet_data, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\ntrain_ds = QuartetDataset(train_data)\nval_ds   = QuartetDataset(val_data)\ntest_ds  = QuartetDataset(test_data)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=32)\ntest_loader  = DataLoader(test_ds, batch_size=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nhistory = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n\nfor epoch in range(1, 40):\n    # --- training loop ---\n    model.train()\n    total_loss, correct = 0, 0\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (logits.argmax(dim=1) == y).sum().item()\n\n    train_loss = total_loss / len(train_loader.dataset)\n    train_acc = correct / len(train_loader.dataset)\n\n    # --- validation loop ---\n    model.eval()\n    total_loss, correct = 0, 0\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            total_loss += loss.item() * x.size(0)\n            correct += (logits.argmax(dim=1) == y).sum().item()\n\n    val_loss = total_loss / len(val_loader.dataset)\n    val_acc = correct / len(val_loader.dataset)\n\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\n    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n\n# --- test evaluation + confusion matrix ---\nmodel.eval()\ntotal_loss, correct = 0, 0\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for x, y in test_loader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss = criterion(logits, y)\n\n        total_loss += loss.item() * x.size(0)\n        correct += (logits.argmax(dim=1) == y).sum().item()\n\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(y.cpu().numpy())\n\ntest_loss = total_loss / len(test_loader.dataset)\ntest_acc = correct / len(test_loader.dataset)\nprint(f\"\\nFinal Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n\n# --- confusion matrix ---\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.show()\n\n# --- plots for training curves ---\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(history[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(1,2,2)\nplt.plot(history[\"train_acc\"], label=\"Train Acc\")\nplt.plot(history[\"val_acc\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:40:18.497857Z","iopub.execute_input":"2025-08-20T17:40:18.498245Z"}},"outputs":[{"name":"stdout","text":"[Data] Found 10000 MSA files and 10000 label files\n","output_type":"stream"},{"name":"stderr","text":"Loading Quartets: 100%|██████████| 10000/10000 [00:15<00:00, 627.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=1.1000, Acc=0.3391 | Val Loss=1.1046, Acc=0.3320\nEpoch 2: Train Loss=1.1000, Acc=0.3376 | Val Loss=1.0991, Acc=0.3213\nEpoch 3: Train Loss=1.0991, Acc=0.3341 | Val Loss=1.0984, Acc=0.3180\nEpoch 4: Train Loss=1.0995, Acc=0.3384 | Val Loss=1.1010, Acc=0.3187\nEpoch 5: Train Loss=1.0989, Acc=0.3431 | Val Loss=1.1001, Acc=0.3207\nEpoch 6: Train Loss=1.0993, Acc=0.3331 | Val Loss=1.0994, Acc=0.3253\nEpoch 7: Train Loss=1.0985, Acc=0.3399 | Val Loss=1.0988, Acc=0.3267\nEpoch 8: Train Loss=1.0981, Acc=0.3474 | Val Loss=1.0982, Acc=0.3467\nEpoch 9: Train Loss=1.0981, Acc=0.3410 | Val Loss=1.0982, Acc=0.3420\nEpoch 10: Train Loss=1.0970, Acc=0.3513 | Val Loss=1.0973, Acc=0.3833\nEpoch 11: Train Loss=1.0955, Acc=0.3643 | Val Loss=1.0956, Acc=0.3427\nEpoch 12: Train Loss=1.0850, Acc=0.3969 | Val Loss=1.0591, Acc=0.4533\nEpoch 13: Train Loss=0.9582, Acc=0.5794 | Val Loss=0.8795, Acc=0.6580\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from Bio import AlignIO\nfrom itertools import combinations\n\ndef generate_quartet_msas(msa_path, format='fasta'):\n    alignment = AlignIO.read(open(msa_path), format)\n    len_of_msa = len(alignment[0].seq)\n    taxa_num = len(alignment)\n    comb_of_id = list(combinations(range(taxa_num), 4))\n    quartet_msas = []\n    for quartet in comb_of_id:\n        records = [alignment[i] for i in quartet]\n        quartet_msas.append(records)\n    return quartet_msas  # List of lists of SeqRecord objects\n\nprocess_msa(\"/kaggle/input/sample-msa/a.fas\")\nquartet_msas = generate_quartet_msas(\"/kaggle/input/sample-msa/a.fas\", format=\"fasta\")\nprint(f\"Number of quartets: {len(quartet_msas)}\")\nprint(f\"Quartet #0: {quartet_msas[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T19:33:46.689915Z","iopub.execute_input":"2025-08-15T19:33:46.690201Z","iopub.status.idle":"2025-08-15T19:33:46.792614Z","shell.execute_reply.started":"2025-08-15T19:33:46.690183Z","shell.execute_reply":"2025-08-15T19:33:46.791824Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Number of quartets: 70\nQuartet #0: [SeqRecord(seq=Seq('AATCAGAAACTCACTCACCTCATGCCGAATCGAAACACATGCGCAACCGAACAG...GAT'), id='A1B2C1D1', name='A1B2C1D1', description='A1B2C1D1', dbxrefs=[]), SeqRecord(seq=Seq('AATCAGAAACTCACTCACCTCATGCCGAATCGAAACACATGCGCAACCGAACAG...GAT'), id='A1B2C1D2', name='A1B2C1D2', description='A1B2C1D2', dbxrefs=[]), SeqRecord(seq=Seq('AATCAGAAACTCACTCACCTCATGCCGAATCGAAACACATGCGCAACCGAACAG...GAT'), id='A1B2C2D1', name='A1B2C2D1', description='A1B2C2D1', dbxrefs=[]), SeqRecord(seq=Seq('AATCAGAAACTCACTCACCTCATGCCGAATCGAAACACATGCGCAACCGAACAG...GAT'), id='A1B2C2D2', name='A1B2C2D2', description='A1B2C2D2', dbxrefs=[])]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for quartet in quartet_msas:\n    msa = [str(record.seq) for record in quartet]\n    input_tensor = encode_msa(msa).unsqueeze(0).to(device)  # shape: (1, 4, L)\n    taxa = [record.id for record in quartet]\n    print(taxa)\n    with torch.no_grad():\n        logits = model(input_tensor)  # shape: (1, 3)\n        prediction = torch.argmax(logits, dim=1).item()  # scalar\n\n    A = taxa[0]\n    B = taxa[1]\n    C = taxa[2]\n    D = taxa[3]\n    topology_map = {\n        0: f\"(({A},{B}),({C},{D}))\",\n        1: f\"(({A},{C}),({B},{D}))\",\n        2: f\"(({A},{D}),({B},{C}))\"\n    }\n    print(\"Predicted topology:\", topology_map[prediction])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T19:33:46.793317Z","iopub.execute_input":"2025-08-15T19:33:46.793515Z","iopub.status.idle":"2025-08-15T19:33:48.178434Z","shell.execute_reply.started":"2025-08-15T19:33:46.793500Z","shell.execute_reply":"2025-08-15T19:33:48.177569Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D1', 'A1B2C2D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B2C1D2,A1B2C2D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D1', 'A1B1C1D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B2C1D2,A1B1C1D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B2C1D2,A1B1C1D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B2C1D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B2C1D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D2', 'A1B1C1D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C1D2,A1B1C1D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D2', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C1D2,A1B1C1D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C1D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B2C2D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C1D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C1D2),(A1B1C1D1,A1B1C1D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C1D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C1D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C1D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C1D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C2D1),(A1B2C1D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C1D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C2D1,A1B1C1D1))\n['A1B2C1D1', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C2D1,A1B1C1D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D1', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D1),(A1B1C1D1,A1B1C1D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C2D1),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D1,A1B2C2D2),(A1B1C1D1,A1B1C1D2))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C2D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C2D2,A1B1C2D1))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D1', 'A1B2C2D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C2D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D1', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B1C1D1,A1B1C2D1))\n['A1B2C1D1', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C1D2),(A1B1C1D1,A1B1C2D2))\n['A1B2C1D1', 'A1B1C1D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C2D1),(A1B1C1D1,A1B1C2D2))\n['A1B2C1D1', 'A1B1C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D1,A1B1C2D1),(A1B1C1D2,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C1D1']\nPredicted topology: ((A1B2C1D2,A1B2C2D2),(A1B2C2D1,A1B1C1D1))\n['A1B2C1D2', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D2,A1B2C2D2),(A1B2C2D1,A1B1C1D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B2C2D2),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D2', 'A1B2C2D1', 'A1B2C2D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B2C2D2),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D2,A1B2C2D1),(A1B1C1D1,A1B1C1D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B1C1D1),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C1D1),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B2C2D1,A1B1C2D1))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C2D1),(A1B2C2D1,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C1D2,A1B2C2D2),(A1B1C1D1,A1B1C1D2))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B1C1D1),(A1B2C2D2,A1B1C2D1))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C1D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B2C2D2,A1B1C2D1))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D2', 'A1B2C2D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C2D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C1D2', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B1C1D1,A1B1C2D1))\n['A1B2C1D2', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C1D2),(A1B1C1D1,A1B1C2D2))\n['A1B2C1D2', 'A1B1C1D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C2D1),(A1B1C1D1,A1B1C2D2))\n['A1B2C1D2', 'A1B1C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C1D2,A1B1C2D1),(A1B1C1D2,A1B1C2D2))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C1D2']\nPredicted topology: ((A1B2C2D1,A1B2C2D2),(A1B1C1D1,A1B1C1D2))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D1']\nPredicted topology: ((A1B2C2D1,A1B1C1D1),(A1B2C2D2,A1B1C2D1))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C1D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C1D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C2D1,A1B1C1D2),(A1B2C2D2,A1B1C2D1))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C1D2),(A1B2C2D2,A1B1C2D2))\n['A1B2C2D1', 'A1B2C2D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C2D1),(A1B2C2D2,A1B1C2D2))\n['A1B2C2D1', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C2D1,A1B1C1D2),(A1B1C1D1,A1B1C2D1))\n['A1B2C2D1', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C1D2),(A1B1C1D1,A1B1C2D2))\n['A1B2C2D1', 'A1B1C1D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C2D1),(A1B1C1D1,A1B1C2D2))\n['A1B2C2D1', 'A1B1C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D1,A1B1C2D1),(A1B1C1D2,A1B1C2D2))\n['A1B2C2D2', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D1']\nPredicted topology: ((A1B2C2D2,A1B1C1D2),(A1B1C1D1,A1B1C2D1))\n['A1B2C2D2', 'A1B1C1D1', 'A1B1C1D2', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D2,A1B1C1D2),(A1B1C1D1,A1B1C2D2))\n['A1B2C2D2', 'A1B1C1D1', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D2,A1B1C2D1),(A1B1C1D1,A1B1C2D2))\n['A1B2C2D2', 'A1B1C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B2C2D2,A1B1C2D1),(A1B1C1D2,A1B1C2D2))\n['A1B1C1D1', 'A1B1C1D2', 'A1B1C2D1', 'A1B1C2D2']\nPredicted topology: ((A1B1C1D1,A1B1C2D1),(A1B1C1D2,A1B1C2D2))\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"id_for_taxa = [i for i in range(0, taxa_num)]\ncomb_of_id = list(combinations(id_for_taxa, 4))\ncomb_of_id.sort(key=lambda ele: ele[-1])\ndl_predict = np.zeros((len(comb_of_id), 3))\n\nfor idx, quartet in enumerate(quartet_msas):\n    msa = [str(record.seq) for record in quartet]\n    input_tensor = encode_msa(msa).unsqueeze(0).to(device)  # shape: (1, 4, L)\n    taxa = [record.id for record in quartet]\n    with torch.no_grad():\n        logits = model(input_tensor)  # shape: (1, 3)\n        prediction = torch.argmax(logits, dim=1).item()  # scalar\n\n    # Fill dl_predict as one-hot\n    dl_predict[idx, prediction] = 1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T19:33:48.179319Z","iopub.execute_input":"2025-08-15T19:33:48.179590Z","iopub.status.idle":"2025-08-15T19:33:48.218218Z","shell.execute_reply.started":"2025-08-15T19:33:48.179559Z","shell.execute_reply":"2025-08-15T19:33:48.217030Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2654764684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid_for_taxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxa_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomb_of_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_for_taxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcomb_of_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdl_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb_of_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'taxa_num' is not defined"],"ename":"NameError","evalue":"name 'taxa_num' is not defined","output_type":"error"}],"execution_count":23}]}